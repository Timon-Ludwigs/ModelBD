import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import sys
import random
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import numpy as np
from tqdm.auto import tqdm
from torch.utils.data import Dataset, DataLoader
import json
import matplotlib.pyplot as plt
from datetime import datetime
import os
sys.path.append(".")
from discrete_diffusion_language_models.src.tokenizer import CustomTokenizer

@dataclass
class ReactionMD4Config:
    """Configuration for Reaction Product Generation MD4 with remasking."""
    vocab_size: int = 1447  
    max_seq_length: int = 230 
    d_model: int = 512
    n_layers: int = 6
    n_heads: int = 8
    d_ff: int = 2048
    dropout: float = 0.1
    mask_token_id: int = -1
    reactant_sep_token_id: int = 23
    pad_token_id: int = 0
    eos_token_id: int = 2
    noagent_token_id: int = 4
    max_product_tokens: int = 82
    noise_schedule: str = "cosine"
    timesteps: int = 1000
    beta_min: float = 1e-4
    beta_max: float = 20.0
    # Remasking parameters
    sigma_base: float = 1.0  # Base remasking rate
    psi: float = 2.0  # Confidence weighting parameter
    eta_conf_scale: float = 1.0  # Scaling factor for confidence-based remasking
    
    def __post_init__(self):
        if self.mask_token_id == -1:
            self.mask_token_id = self.vocab_size


class ReactionMD4Remasking(nn.Module):
    """
    Reaction Product Generation using Masked Diffusion with Remasking and Inference Scaling.
    Implements L^σ loss and confidence-based remasking schedule.
    """
    
    def __init__(self, config: ReactionMD4Config):
        super().__init__()
        self.config = config
        
        # Token embeddings (including mask token)
        self.token_embedding = nn.Embedding(config.vocab_size + 1, config.d_model)
        
        # Position encoding
        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length * 2)
        
        # Time embedding
        self.time_embedding = TimeEmbedding(config.d_model)
        
        # Transformer backbone
        self.layers = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        
        self.ln_f = nn.LayerNorm(config.d_model)
        
        # Output head - predict original tokens
        self.output_head = nn.Linear(config.d_model, config.vocab_size)
        
        self.dropout = nn.Dropout(config.dropout)
        
        # Precompute noise schedule
        self._precompute_noise_schedule()
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """Initialize weights following standard transformer initialization."""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def _precompute_noise_schedule(self):
        """Precompute the noise schedule parameters."""
        timesteps = self.config.timesteps
        
        if self.config.noise_schedule == "linear":
            betas = torch.linspace(self.config.beta_min, self.config.beta_max, timesteps)
        elif self.config.noise_schedule == "cosine":
            s = 0.008
            steps = torch.arange(timesteps + 1, dtype=torch.float32) / timesteps
            alphas_cumprod = torch.cos((steps + s) / (1 + s) * math.pi * 0.5) ** 2
            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
            betas = torch.clamp(betas, 0.0001, 0.9999)
        else:  # geometric
            betas = torch.exp(torch.linspace(
                math.log(self.config.beta_min), 
                math.log(self.config.beta_max), 
                timesteps
            ))
        
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, dim=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
        
        self.register_buffer('betas', betas)
        self.register_buffer('alphas', alphas)
        self.register_buffer('alphas_cumprod', alphas_cumprod)
        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)
    
    def get_masking_probability_at_t(self, t: torch.Tensor) -> torch.Tensor:
        """Get the masking probability at timestep t."""
        t_indices = (t * (self.config.timesteps - 1)).long()
        t_indices = torch.clamp(t_indices, 0, self.config.timesteps - 1)
        alpha_t = self.alphas_cumprod.gather(0, t_indices)
        return alpha_t
    
    def compute_remasking_loss(self, logits: torch.Tensor, x_target: torch.Tensor, 
                               x_noisy: torch.Tensor, t: torch.Tensor, 
                               product_mask: torch.Tensor, is_masked: torch.Tensor,
                               sigma_t: float = 1.0) -> Dict[str, torch.Tensor]:
        """
        Compute the L^σ loss with remasking as described in the paper.
        
        L^σ = E[-log p_θ(x|z_0)] + E[E[I[((1-σ_t)α_t - α_s)/(1-α_t) * log(x_θ^T x)]]
        
        Args:
            logits: Model predictions [batch, seq_len, vocab_size]
            x_target: Original target sequence (x_0) [batch, seq_len]
            x_noisy: Noisy input sequence (z_t) [batch, seq_len]
            t: Timestep in [0, 1] [batch]
            product_mask: Mask for product positions [batch, seq_len]
            is_masked: Boolean indicating which positions are masked [batch, seq_len]
            sigma_t: Remasking rate parameter
        """
        batch_size, seq_len, vocab_size = logits.shape
        device = logits.device
        
        # Convert continuous t to discrete timesteps
        t_indices = (t * (self.config.timesteps - 1)).long()
        t_indices = torch.clamp(t_indices, 0, self.config.timesteps - 1)
        
        # Get alpha values
        alpha_t = self.alphas_cumprod.gather(0, t_indices)  # [batch]
        
        # For s = t-1 (previous timestep)
        alpha_s = torch.where(
            t_indices > 0,
            self.alphas_cumprod.gather(0, (t_indices - 1).clamp(min=0)),
            torch.ones_like(alpha_t)
        )
        
        # Valid positions for loss computation
        valid_positions = product_mask & (x_target != self.config.pad_token_id)
        
        # 1. Reconstruction term: -log p_θ(x|z_0)
        # This is the standard cross-entropy loss on masked positions
        reconstruction_loss = torch.tensor(0.0, device=device, requires_grad=True)
        masked_valid = valid_positions & is_masked
        
        if masked_valid.any():
            ce_loss = F.cross_entropy(
                logits[masked_valid],
                x_target[masked_valid],
                reduction='none'
            )
            reconstruction_loss = ce_loss.mean()
        
        # 2. Remasking term with σ_t
        # E[I[((1-σ_t)α_t - α_s)/(1-α_t) * log(x_θ^T x)]]
        remasking_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        if masked_valid.any():
            # Compute remasking probability with sigma
            remask_prob = ((1 - sigma_t) * alpha_t - alpha_s) / (1 - alpha_t + 1e-8)
            remask_prob = torch.clamp(remask_prob, 0.0, 1.0)  # [batch]
            
            # Get log probabilities for true tokens
            log_probs = F.log_softmax(logits, dim=-1)
            true_log_probs = torch.gather(log_probs, -1, x_target.unsqueeze(-1)).squeeze(-1)
            
            # Apply remasking weight
            remask_prob_expanded = remask_prob.unsqueeze(-1).expand(-1, seq_len)
            weighted_log_probs = remask_prob_expanded * true_log_probs
            
            # Only apply to masked positions
            remasking_loss = -weighted_log_probs[masked_valid].mean()
        
        # 3. KL regularization term (optional, for stability)
        kl_loss = torch.tensor(0.0, device=device, requires_grad=True)
        if masked_valid.any():
            # KL divergence from uniform distribution
            uniform_prob = 1.0 / vocab_size
            log_probs = F.log_softmax(logits[masked_valid], dim=-1)
            kl_loss = F.kl_div(
                log_probs,
                torch.ones_like(log_probs) * uniform_prob,
                reduction='batchmean'
            ) * 0.01  # Small weight for regularization
        
        # Combine losses
        total_loss = reconstruction_loss + remasking_loss + kl_loss
        
        return {
            'loss': total_loss,
            'reconstruction_loss': reconstruction_loss,
            'remasking_loss': remasking_loss,
            'kl_loss': kl_loss,
            'sigma_t': sigma_t
        }
    
    def compute_confidence_weights(self, logits: torch.Tensor, t: float) -> torch.Tensor:
        """
        Compute confidence weights for each position based on model predictions.
        
        η_conf = exp(-ψ * conf_l) / Σ_l exp(-ψ * conf_l)
        
        where conf_l is the confidence (max probability) at position l
        """
        probs = F.softmax(logits, dim=-1)
        max_probs, _ = torch.max(probs, dim=-1)  # [batch, seq_len]
        
        # Compute confidence scores with temperature scaling
        psi = self.config.psi * (1 - t)  # Decay psi over time
        confidence_scores = torch.exp(-psi * (1 - max_probs))
        
        # Normalize to get weights
        confidence_weights = confidence_scores / (confidence_scores.sum(dim=-1, keepdim=True) + 1e-8)
        
        return confidence_weights
    
    def forward_sample_with_remasking(self, x_target: torch.Tensor, t: torch.Tensor,
                                     product_mask: torch.Tensor, 
                                     sigma_t: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward diffusion with remasking based on σ_t parameter.
        """
        batch_size, seq_len = x_target.shape
        
        # Get base masking probability
        keep_prob = self.get_masking_probability_at_t(t)
        
        # Apply sigma scaling for remasking
        # σ_t = 1 means standard masking, σ_t < 1 means less masking
        effective_keep_prob = keep_prob + (1 - keep_prob) * (1 - sigma_t)
        effective_keep_prob = torch.clamp(effective_keep_prob, 0.0, 1.0)
        effective_keep_prob = effective_keep_prob.unsqueeze(-1)
        
        # Sample which positions to keep unmasked
        random_probs = torch.rand_like(x_target, dtype=torch.float)
        should_keep = random_probs < effective_keep_prob
        
        # Only apply masking to product positions
        should_mask = product_mask & ~should_keep
        
        x_noisy = torch.where(should_mask, self.config.mask_token_id, x_target)
        
        return x_noisy, should_mask
    
    def forward(self, input_ids: torch.Tensor, t: torch.Tensor,
                target_ids: Optional[torch.Tensor] = None,
                product_mask: Optional[torch.Tensor] = None,
                sigma_t: float = 1.0) -> Dict[str, torch.Tensor]:
        """
        Forward pass with remasking loss computation.
        """
        # Token embeddings
        token_emb = self.token_embedding(input_ids)
        
        # Add positional encoding
        x = self.pos_encoding(token_emb)
        x = self.dropout(x)
        
        # Time embedding
        time_emb = self.time_embedding(t)
        
        # Apply transformer layers
        for layer in self.layers:
            x = layer(x, time_emb)
        
        x = self.ln_f(x)
        
        # Output logits
        logits = self.output_head(x)
        
        outputs = {"logits": logits}
        
        # Compute remasking loss during training
        if target_ids is not None and product_mask is not None:
            is_masked = (input_ids == self.config.mask_token_id)
            
            if product_mask.any() and is_masked.any():
                loss_outputs = self.compute_remasking_loss(
                    logits, target_ids, input_ids, t, product_mask, is_masked, sigma_t
                )
                outputs.update(loss_outputs)
            else:
                outputs["loss"] = torch.tensor(0.0, device=input_ids.device, requires_grad=True)
        
        return outputs
    
    def sample_step_with_remasking(self, xt: torch.Tensor, t_curr: torch.Tensor, 
                                  t_next: torch.Tensor, product_mask: torch.Tensor,
                                  temperature: float = 1.0, 
                                  use_confidence_remasking: bool = True) -> torch.Tensor:
        """
        Single sampling step with confidence-based remasking.
        """
        batch_size = xt.shape[0]
        
        # Get model predictions
        outputs = self(xt, t_curr)
        logits = outputs["logits"] / temperature
        
        # Compute confidence weights if enabled
        if use_confidence_remasking:
            confidence_weights = self.compute_confidence_weights(logits, t_curr[0].item())
            # Apply confidence-based sigma
            sigma_t = self.config.sigma_base * self.config.eta_conf_scale
            eta_conf = confidence_weights  # [batch, seq_len]
        else:
            sigma_t = self.config.sigma_base
            eta_conf = torch.ones_like(xt, dtype=torch.float) / xt.shape[1]
        
        # Get transition probabilities with remasking
        alpha_t = self.get_masking_probability_at_t(t_curr).unsqueeze(-1)
        alpha_s = self.get_masking_probability_at_t(t_next).unsqueeze(-1)
        
        # Modified unmasking probability with sigma
        p_unmask = ((1 - sigma_t) * alpha_s - alpha_t) / (1 - alpha_t + 1e-8)
        p_unmask = torch.clamp(p_unmask, 0.0, 1.0)
        
        # Apply confidence weighting to unmasking probability
        if use_confidence_remasking:
            p_unmask = p_unmask * eta_conf
        
        # Identify positions that can be unmasked
        is_mask = (xt == self.config.mask_token_id)
        can_unmask = is_mask & product_mask
        
        if can_unmask.any():
            # Decide which positions to unmask
            unmask_probs = torch.rand_like(xt, dtype=torch.float)
            positions_to_unmask = can_unmask & (unmask_probs < p_unmask)
            
            if positions_to_unmask.any():
                # Sample new tokens for unmasked positions
                probs = F.softmax(logits[positions_to_unmask], dim=-1)
                sampled_tokens = torch.multinomial(probs, 1).squeeze(-1)
                
                xt = xt.clone()
                xt[positions_to_unmask] = sampled_tokens
                
            # Remasking step: remask some positions based on confidence
            if use_confidence_remasking and t_curr[0] > 0.1:  # Don't remask near the end
                # Positions that could be remasked (currently unmasked in product region)
                can_remask = ~is_mask & product_mask
                
                if can_remask.any():
                    # Use inverse confidence for remasking probability
                    remask_prob = (1 - eta_conf) * sigma_t * 0.1  # Small remasking rate
                    remask_random = torch.rand_like(xt, dtype=torch.float)
                    positions_to_remask = can_remask & (remask_random < remask_prob)
                    
                    if positions_to_remask.any():
                        xt[positions_to_remask] = self.config.mask_token_id
        
        return xt
    
    @torch.no_grad()
    def sample(self, reactants_list: List[str], agents_list: List[str], tokenizer,
               num_steps: int = 100, temperature: float = 1.0,
               use_confidence_remasking: bool = True) -> List[torch.Tensor]:
        """
        Generate products using reverse diffusion with confidence-based remasking.
        """
        batch_size = len(reactants_list)
        if batch_size == 0:
            return []
            
        device = next(self.parameters()).device
        
        # Create initial sequences with masked product regions
        initial_seqs = []
        product_masks = []
        product_starts = []
        
        for reactants, agents in zip(reactants_list, agents_list):
            input_seq, product_start = create_inference_input(
                reactants, agents, tokenizer, self.config
            )
            initial_seqs.append(input_seq)
            product_starts.append(product_start)
            
            # Create product mask
            product_mask = torch.zeros(self.config.max_seq_length, dtype=torch.bool)
            product_mask[product_start:-1] = True
            product_masks.append(product_mask)
        
        # Stack tensors
        xt = torch.stack(initial_seqs).to(device)
        product_mask = torch.stack(product_masks).to(device)
        
        # Create sampling schedule
        timesteps = torch.linspace(1.0, 0.0, num_steps + 1, device=device)
        
        # Reverse diffusion with remasking
        for i in range(num_steps):
            t_curr = timesteps[i].expand(batch_size)
            t_next = timesteps[i + 1] if i < num_steps - 1 else torch.zeros_like(t_curr)
            
            xt = self.sample_step_with_remasking(
                xt, t_curr, t_next, product_mask, temperature, use_confidence_remasking
            )
        
        # Extract generated products
        generated_products = []
        for i in range(batch_size):
            product_start = product_starts[i]
            product_tokens = xt[i, product_start:]
            
            # Remove padding, mask tokens, and find EOS
            valid_tokens = []
            for token in product_tokens:
                token_id = token.item()
                if token_id in [self.config.pad_token_id, self.config.mask_token_id]:
                    continue
                elif token_id == self.config.eos_token_id:
                    break
                else:
                    valid_tokens.append(token_id)
            
            generated_products.append(torch.tensor(valid_tokens, device=device))
        
        return generated_products
    
    def generate_product(self, reactants: str, agents: str, tokenizer,
                        num_steps: int = 100, temperature: float = 1.0,
                        use_confidence_remasking: bool = True) -> str:
        """Generate a single reaction product with remasking."""
        generated_products = self.sample(
            [reactants], [agents], tokenizer, num_steps, temperature, use_confidence_remasking
        )
        
        if generated_products and len(generated_products[0]) > 0:
            product_tokens = generated_products[0].cpu().tolist()
            return tokenizer.decode(product_tokens)
        else:
            return ""


# Helper classes remain the same
class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding."""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].unsqueeze(0)


class TimeEmbedding(nn.Module):
    """Time embedding for diffusion timestep."""
    
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model)
        )
        
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        # Sinusoidal embedding
        half_dim = self.d_model // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)
        emb = t[:, None] * emb[None, :]
        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)
        
        return self.mlp(emb)


class TransformerBlock(nn.Module):
    """Transformer block with time conditioning."""
    
    def __init__(self, config: ReactionMD4Config):
        super().__init__()
        self.config = config
        
        self.ln1 = nn.LayerNorm(config.d_model)
        self.attn = nn.MultiheadAttention(
            config.d_model, config.n_heads, dropout=config.dropout,
            batch_first=True
        )
        self.ln2 = nn.LayerNorm(config.d_model)
        
        # Time-conditioned MLP
        self.time_mlp = nn.Linear(config.d_model, config.d_model)
        self.mlp = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_model),
            nn.Dropout(config.dropout)
        )
        
    def forward(self, x: torch.Tensor, time_emb: torch.Tensor, 
                attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention
        x_norm = self.ln1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=attn_mask)
        x = x + attn_out
        
        # Time-conditioned MLP
        x_norm = self.ln2(x)
        time_cond = self.time_mlp(time_emb).unsqueeze(1)
        x_norm = x_norm + time_cond
        mlp_out = self.mlp(x_norm)
        x = x + mlp_out
        
        return x

def preprocess_reaction_for_training(reaction_str: str, tokenizer, config: ReactionMD4Config):
    """Preprocess reaction for training - masks only the product part."""
    parts = reaction_str.split(">")
    
    if len(parts) == 3:
        reactants, agents, product = parts
    elif len(parts) == 2:
        reactants, product = parts
        agents = tokenizer.noagent_token
    else:
        raise ValueError(f"Unexpected reaction format: {reaction_str}")
    
    react_ids = tokenizer.encode(reactants)
    agents_ids = tokenizer.encode(agents)
    prod_ids = tokenizer.encode(product)
    
    sep_id = tokenizer.get_special_token_id(">")
    x_target = (
        react_ids + [sep_id] +
        agents_ids + [sep_id] +
        prod_ids + [tokenizer.eos_token_id]
    )
    
    x_masked = (
        react_ids + [sep_id] +
        agents_ids + [sep_id] +
        [config.mask_token_id] * len(prod_ids) + 
        [tokenizer.eos_token_id]
    )
    
    if len(x_target) > config.max_seq_length:
        x_target = x_target[:config.max_seq_length]
        x_masked = x_masked[:config.max_seq_length]
    
    pad_len = config.max_seq_length - len(x_target)
    x_target += [config.pad_token_id] * pad_len
    x_masked += [config.pad_token_id] * pad_len
    
    product_mask = [False] * config.max_seq_length
    second_sep_idx = len(react_ids) + 1 + len(agents_ids) + 1  
    
    for i in range(second_sep_idx, min(second_sep_idx + len(prod_ids), config.max_seq_length)):
        product_mask[i] = True
    
    return torch.tensor(x_masked), torch.tensor(x_target), torch.tensor(product_mask)

def create_inference_input(reactants: str, agents: str, tokenizer, config: ReactionMD4Config):
    """Create input for inference: reactants>agents>[MASK]...[MASK]"""
    if not agents:
        agents = tokenizer.noagent_token
    
    react_ids = tokenizer.encode(reactants)
    agents_ids = tokenizer.encode(agents)
    sep_id = tokenizer.get_special_token_id(">")
    
    prefix = react_ids + [sep_id] + agents_ids + [sep_id]
    
    min_required_space = 2
    if len(prefix) >= config.max_seq_length - min_required_space:
        max_prefix_len = config.max_seq_length - min_required_space
        prefix = prefix[:max_prefix_len]
    
    product_start_idx = len(prefix)
    n_mask_tokens = config.max_seq_length - len(prefix) - 1
    
    if n_mask_tokens <= 0:
        raise ValueError(f"Cannot fit sequence: prefix_len={len(prefix)}, max_seq_length={config.max_seq_length}")
    
    input_seq = prefix + [config.mask_token_id] * n_mask_tokens + [tokenizer.eos_token_id]
    
    if len(input_seq) < config.max_seq_length:
        input_seq += [config.pad_token_id] * (config.max_seq_length - len(input_seq))
    
    return torch.tensor(input_seq), product_start_idx

class ReactionDataset(Dataset):
    """Dataset for reaction SMILES strings."""
    
    def __init__(self, data: List[str], tokenizer, config: ReactionMD4Config):
        self.data = data
        self.tokenizer = tokenizer
        self.config = config

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        reaction_str = self.data[idx].strip()
        
        # Preprocess reaction for training
        x_masked, x_target, product_mask = preprocess_reaction_for_training(
            reaction_str, self.tokenizer, self.config
        )
        
        return {
            "x_masked": x_masked,
            "x_target": x_target,
            "product_mask": product_mask
        }


def calculate_accuracy_fixed(predictions: torch.Tensor, targets: torch.Tensor, 
                            mask: torch.Tensor, pad_token_id: int, 
                            noisy_input: torch.Tensor, mask_token_id: int) -> Dict[str, float]:
    """Calculate token-wise and sequence-wise accuracy - FIXED VERSION."""
    
    # Only consider positions that are:
    # 1. In the product mask (product region)
    # 2. Not padding tokens  
    # 3. Actually masked in the noisy input (what the model needs to predict)
    actually_masked = (noisy_input == mask_token_id)
    valid_positions = mask & (targets != pad_token_id) & actually_masked
    
    if not valid_positions.any():
        return {"token_accuracy": 0.0, "sequence_accuracy": 0.0}
    
    # Token-wise accuracy - only on positions the model actually predicted
    correct_tokens = (predictions == targets) & valid_positions
    token_accuracy = correct_tokens.sum().float() / valid_positions.sum().float()
    
    # Sequence-wise accuracy - all masked tokens in each sequence must be correct
    batch_size = predictions.shape[0]
    sequence_correct = torch.zeros(batch_size, dtype=torch.bool, device=predictions.device)
    
    for i in range(batch_size):
        seq_mask = valid_positions[i]
        if seq_mask.any():
            seq_correct = (predictions[i] == targets[i]) & seq_mask
            sequence_correct[i] = seq_correct.sum() == seq_mask.sum()
        else:
            # If no tokens to predict, consider it "correct"
            sequence_correct[i] = True
    
    sequence_accuracy = sequence_correct.float().mean()
    
    return {
        "token_accuracy": token_accuracy.item(),
        "sequence_accuracy": sequence_accuracy.item(),
        "num_predicted_tokens": valid_positions.sum().item()
    }

def train_reaction_md4_with_tracking(model: ReactionMD4Remasking, train_dataset, val_dataset, tokenizer,
                                   num_epochs: int = 10, batch_size: int = 32,
                                   learning_rate: float = 1e-4, validation_freq: int = 1,
                                   plot_freq: int = 5, save_plots: bool = True, plots_dir: str = "training_plots"):
    """Enhanced training loop with metrics tracking and plotting."""
    
    device = next(model.parameters()).device
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))
    
    # Initialize metrics tracker
    metrics_tracker = MetricsTracker()
    
    best_val_loss = float('inf')
    
    print(f"Starting training for {num_epochs} epochs...")
    print(f"Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}")
    print("=" * 80)
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_token_acc = 0
        train_seq_acc = 0
        num_successful_batches = 0
        
        pbar_train = tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch+1}/{num_epochs} [Training]",
            ncols=120,
            leave=True,
            position=0
        )
        
        for batch_idx, batch in enumerate(pbar_train):
            x_masked = batch['x_masked'].to(device)
            x_target = batch['x_target'].to(device) 
            product_mask = batch['product_mask'].to(device)
            
            batch_size_curr = x_masked.shape[0]
            
            # Sample random time steps
            t = torch.rand(batch_size_curr, device=device)
            
            # Forward diffusion (add noise to target based on schedule)
            x_noisy, is_masked = model.forward_sample_with_remasking(x_target, t, product_mask)
            
            # Forward pass
            outputs = model(x_noisy, t, x_target, product_mask)
            loss = outputs.get('loss', torch.tensor(0.0, device=device, requires_grad=True))
            
            # Skip if no valid loss
            if loss.item() <= 0 or not loss.requires_grad:
                continue
            
            # Calculate accuracy
            if product_mask.any():
                with torch.no_grad():
                    logits = outputs["logits"]
                    predictions = torch.argmax(logits, dim=-1)
                            
                    # Use the fixed accuracy calculation
                    acc_metrics = calculate_accuracy_fixed(
                        predictions, x_target, product_mask, 
                        model.config.pad_token_id, x_noisy, model.config.mask_token_id
                    )
                    train_token_acc += acc_metrics["token_accuracy"]
                    train_seq_acc += acc_metrics["sequence_accuracy"]

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            num_successful_batches += 1
            
            # Update progress bar
            if num_successful_batches > 0:
                avg_train_loss = train_loss / num_successful_batches
                avg_token_acc = train_token_acc / num_successful_batches
                avg_seq_acc = train_seq_acc / num_successful_batches
                
                pbar_train.set_postfix({
                    "loss": f"{avg_train_loss:.4f}",
                    "tok_acc": f"{avg_token_acc:.3f}",
                    "seq_acc": f"{avg_seq_acc:.3f}",
                    "lr": f"{scheduler.get_last_lr()[0]:.2e}"
                })
        
        pbar_train.close()
        
        # Calculate final training metrics
        if num_successful_batches > 0:
            avg_train_loss = train_loss / num_successful_batches
            avg_train_token_acc = train_token_acc / num_successful_batches
            avg_train_seq_acc = train_seq_acc / num_successful_batches
        else:
            avg_train_loss = avg_train_token_acc = avg_train_seq_acc = 0.0
        
        current_lr = scheduler.get_last_lr()[0]
        
        # Validation phase
        avg_val_loss = avg_val_token_acc = avg_val_seq_acc = None
        
        if (epoch + 1) % validation_freq == 0:
            model.eval()
            val_loss = 0
            val_token_acc = 0
            val_seq_acc = 0
            num_val_batches = 0
            
            pbar_val = tqdm(
                val_dataloader, 
                desc=f"Epoch {epoch+1}/{num_epochs} [Validation]",
                ncols=120,
                leave=True,
                position=0
            )
            
            with torch.no_grad():
                for batch in pbar_val:
                    x_masked = batch['x_masked'].to(device)
                    x_target = batch['x_target'].to(device)
                    product_mask = batch['product_mask'].to(device)
                    
                    batch_size_curr = x_masked.shape[0]
                    
                    # Sample random time steps for validation
                    t = torch.rand(batch_size_curr, device=device)
                    
                    # Forward diffusion
                    x_noisy, is_masked = model.forward_sample_with_remasking(x_target, t, product_mask)
                    
                    # Forward pass
                    outputs = model(x_noisy, t, x_target, product_mask)
                    loss = outputs.get('loss', torch.tensor(0.0, device=device))
                    
                    # Calculate accuracy
                    if product_mask.any():
                        with torch.no_grad():
                            logits = outputs["logits"]
                            predictions = torch.argmax(logits, dim=-1)
                            
                            # Use the fixed accuracy calculation
                            acc_metrics = calculate_accuracy_fixed(
                                predictions, x_target, product_mask, 
                                model.config.pad_token_id, x_noisy, model.config.mask_token_id
                            )
                            val_token_acc += acc_metrics["token_accuracy"]
                            val_seq_acc += acc_metrics["sequence_accuracy"]
                    
                    val_loss += loss.item()
                    num_val_batches += 1
                    
                    # Update validation progress bar
                    if num_val_batches > 0:
                        avg_val_loss_temp = val_loss / num_val_batches
                        avg_val_token_acc_temp = val_token_acc / num_val_batches
                        avg_val_seq_acc_temp = val_seq_acc / num_val_batches
                        
                        pbar_val.set_postfix({
                            "loss": f"{avg_val_loss_temp:.4f}",
                            "tok_acc": f"{avg_val_token_acc_temp:.3f}",
                            "seq_acc": f"{avg_val_seq_acc_temp:.3f}"
                        })
            
            pbar_val.close()
            
            # Calculate final validation metrics
            if num_val_batches > 0:
                avg_val_loss = val_loss / num_val_batches
                avg_val_token_acc = val_token_acc / num_val_batches
                avg_val_seq_acc = val_seq_acc / num_val_batches
            else:
                avg_val_loss = avg_val_token_acc = avg_val_seq_acc = 0.0
        
        # Update metrics tracker
        metrics_tracker.update(
            epoch=epoch + 1,
            train_loss=avg_train_loss,
            train_token_acc=avg_train_token_acc,
            train_seq_acc=avg_train_seq_acc,
            val_loss=avg_val_loss,
            val_token_acc=avg_val_token_acc,
            val_seq_acc=avg_val_seq_acc,
            lr=current_lr
        )
        
        # Print epoch summary
        if avg_val_loss is not None:
            print(f"\n EPOCH {epoch+1} SUMMARY:")
            print(f"   Train → Loss: {avg_train_loss:.4f} | Token Acc: {avg_train_token_acc:.3f} | Seq Acc: {avg_train_seq_acc:.3f}")
            print(f"   Val   → Loss: {avg_val_loss:.4f} | Token Acc: {avg_val_token_acc:.3f} | Seq Acc: {avg_val_seq_acc:.3f}")
            print(f"   LR: {current_lr:.2e}")
            
            # Check for best model
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                print(f"    NEW BEST VALIDATION LOSS! ({avg_val_loss:.4f})")
            
            # Generate example products
            print(f"\n    GENERATED EXAMPLES:")
            generate_examples_during_training(model, val_dataset, tokenizer, num_examples=3)
        else:
            print(f"\n EPOCH {epoch+1} SUMMARY:")
            print(f"   Train → Loss: {avg_train_loss:.4f} | Token Acc: {avg_train_token_acc:.3f} | Seq Acc: {avg_train_seq_acc:.3f}")
            print(f"   LR: {current_lr:.2e}")
        
        # Plot metrics periodically
        if (epoch + 1) % plot_freq == 0 or epoch == num_epochs - 1:
            print(f"\n    PLOTTING METRICS...")
            if save_plots:
                os.makedirs(plots_dir, exist_ok=True)
                plot_path = os.path.join(plots_dir, f"metrics_epoch_{epoch+1}.png")
                metrics_tracker.plot_metrics(save_path=plot_path)
            else:
                metrics_tracker.plot_metrics()
        
        print("=" * 80)
    
    print(f"\n TRAINING COMPLETED!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    
    # Final metrics summary and plots
    print("\n" + "="*60)
    print("FINAL TRAINING SUMMARY")
    print("="*60)
    metrics_tracker.print_summary()
    
    # Generate final comprehensive plots
    if save_plots:
        print(f"\nSaving final plots to {plots_dir}/...")
        os.makedirs(plots_dir, exist_ok=True)
        final_plot_path = os.path.join(plots_dir, "final_training_metrics.png")
        metrics_tracker.plot_metrics(save_path=final_plot_path)
        
        # Also save individual plots
        individual_plots_dir = os.path.join(plots_dir, "individual")
        metrics_tracker.plot_individual_metrics(save_dir=individual_plots_dir)
    else:
        metrics_tracker.plot_metrics()
    
    return metrics_tracker


def generate_examples_during_training(model: ReactionMD4Remasking, dataset, tokenizer, num_examples: int = 3):
    """Generate example products during training to monitor progress."""
    model.eval()
    device = next(model.parameters()).device
    
    # Get some examples from the dataset
    indices = torch.randperm(len(dataset))[:num_examples]
    
    with torch.no_grad():
        for i, idx in enumerate(indices):
            try:
                sample = dataset[idx]
                x_target = sample['x_target']
                
                # Decode the full target reaction
                original_reaction = tokenizer.decode(x_target.tolist())
                
                # Parse reaction to extract reactants and agents
                parts = original_reaction.split('>')
                if len(parts) >= 3:
                    reactants = parts[0]
                    agents = parts[1] if parts[1] else ""
                    true_product = parts[2]
                elif len(parts) == 2:
                    reactants = parts[0]
                    agents = ""
                    true_product = parts[1]
                else:
                    continue
                
                # Generate product
                generated_product = model.generate_product(
                    reactants, agents, tokenizer, num_steps=50, temperature=1.0
                )
                
                # Format output
                reactants_agents_text = f"{reactants}>{agents}>" if agents else f"{reactants}>>"
                match_indicator = "CORRECT" if generated_product.strip() == true_product.strip() else "WRONG"
                
                print(f"      {i+1}. {reactants_agents_text}")
                print(f"         True:      {true_product}")
                print(f"         Generated: {generated_product} {match_indicator}")
                if i < num_examples - 1:
                    print()
            except Exception as e:
                print(f"      {i+1}. Error generating example - {str(e)}")
    
    model.train()


@torch.no_grad()
def test_model(model: ReactionMD4Remasking, test_dataset, tokenizer, 
               batch_size: int = 32, num_steps: int = 100):
    """Test the model on test dataset and return comprehensive metrics."""
    model.eval()
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Use batch_size=1 for simplicity
    
    all_token_accuracies = []
    all_seq_accuracies = []
    all_generated_products = []
    all_true_products = []
    all_inputs = []
    
    print("Running comprehensive test evaluation...")
    
    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="Testing")):
        x_target = batch['x_target'][0]  # Remove batch dimension
        
        # Decode the full target reaction
        original_reaction = tokenizer.decode(x_target.tolist())
        
        # Parse reaction to extract components
        parts = original_reaction.split('>')
        if len(parts) >= 3:
            reactants = parts[0]
            agents = parts[1] if parts[1] else ""
            true_product = parts[2]
        elif len(parts) == 2:
            reactants = parts[0]
            agents = ""
            true_product = parts[1]
        else:
            continue
        
        # Generate product
        generated_product = model.generate_product(
            reactants, agents, tokenizer, num_steps=num_steps, temperature=1.0
        )
        
        # Calculate accuracy at character level (simple metric)
        true_product_clean = true_product.strip()
        generated_product_clean = generated_product.strip()
        
        if true_product_clean:
            # Token-level accuracy (exact match)
            seq_accuracy = 1.0 if generated_product_clean == true_product_clean else 0.0
            
            # Character-level token accuracy (rough approximation)
            if generated_product_clean:
                true_tokens = tokenizer.encode(true_product_clean)
                gen_tokens = tokenizer.encode(generated_product_clean)
                
                # Pad to same length for comparison
                max_len = max(len(true_tokens), len(gen_tokens))
                true_padded = true_tokens + [tokenizer.pad_token_id] * (max_len - len(true_tokens))
                gen_padded = gen_tokens + [tokenizer.pad_token_id] * (max_len - len(gen_tokens))
                
                correct_tokens = sum(1 for t, g in zip(true_padded, gen_padded) if t == g and t != tokenizer.pad_token_id)
                total_tokens = len([t for t in true_tokens if t != tokenizer.pad_token_id])
                token_accuracy = correct_tokens / max(total_tokens, 1)
            else:
                token_accuracy = 0.0
            
            all_token_accuracies.append(token_accuracy)
            all_seq_accuracies.append(seq_accuracy)
            
            # Store for detailed analysis
            all_inputs.append(original_reaction)
            all_true_products.append(true_product_clean)
            all_generated_products.append(generated_product_clean)
        
        if (batch_idx + 1) % 100 == 0:
            print(f"  Processed {batch_idx + 1}/{len(test_dataloader)} samples")
    
    # Calculate final metrics
    if all_token_accuracies:
        final_token_acc = np.mean(all_token_accuracies)
        final_seq_acc = np.mean(all_seq_accuracies)
        
        print(f"\n{'='*50}")
        print("FINAL TEST RESULTS")
        print(f"{'='*50}")
        print(f"Token Accuracy: {final_token_acc:.4f}")
        print(f"Sequence Accuracy: {final_seq_acc:.4f}")
        print(f"Total Test Samples: {len(all_token_accuracies)}")
        
        # Show some examples
        print(f"\nSample Test Results:")
        num_examples = min(5, len(all_inputs))
        for i in range(num_examples):
            print(f"\nExample {i+1}:")
            # Extract input part (reactants+agents)
            parts = all_inputs[i].split('>')
            if len(parts) >= 2:
                input_part = '>'.join(parts[:-1]) + '>'
                print(f"  Input:      {input_part}")
            print(f"  True:       {all_true_products[i]}")
            print(f"  Generated:  {all_generated_products[i]}")
            print(f"  Match:      {'✓' if all_seq_accuracies[i] == 1.0 else '✗'}")
        
        return {
            "token_accuracy": final_token_acc,
            "sequence_accuracy": final_seq_acc,
            "num_samples": len(all_token_accuracies),
            "examples": {
                "inputs": all_inputs[:10],
                "true_products": all_true_products[:10],
                "generated_products": all_generated_products[:10]
            }
        }
    else:
        print("No valid test samples found!")
        return {"token_accuracy": 0.0, "sequence_accuracy": 0.0, "num_samples": 0}


@dataclass
class TrainingMetrics:
    """Container for training metrics."""
    epoch: int
    train_loss: float
    train_token_acc: float
    train_seq_acc: float
    val_loss: float = None
    val_token_acc: float = None
    val_seq_acc: float = None
    learning_rate: float = None
    timestamp: str = None
    generated_examples: List[Dict] = None

class MetricsTracker:
    """Track and store training metrics with visualization."""
    
    def __init__(self, save_dir: str = "training_logs"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        self.metrics_history = []
        self.generated_examples_history = []
        
    def add_metrics(self, metrics: TrainingMetrics):
        """Add metrics for current epoch."""
        metrics.timestamp = datetime.now().isoformat()
        self.metrics_history.append(metrics)
        
    def add_generated_examples(self, epoch: int, examples: List[Dict]):
        """Add generated examples for current epoch."""
        self.generated_examples_history.append({
            "epoch": epoch,
            "timestamp": datetime.now().isoformat(),
            "examples": examples
        })
        
    def plot_metrics(self, save_path: str = None):
        """Create comprehensive training plots."""
        if not self.metrics_history:
            print("No metrics to plot!")
            return
            
        # Extract data
        epochs = [m.epoch for m in self.metrics_history]
        train_losses = [m.train_loss for m in self.metrics_history]
        train_token_accs = [m.train_token_acc for m in self.metrics_history]
        train_seq_accs = [m.train_seq_acc for m in self.metrics_history]
        
        # Validation metrics (may be sparse)
        val_epochs = [m.epoch for m in self.metrics_history if m.val_loss is not None]
        val_losses = [m.val_loss for m in self.metrics_history if m.val_loss is not None]
        val_token_accs = [m.val_token_acc for m in self.metrics_history if m.val_token_acc is not None]
        val_seq_accs = [m.val_seq_acc for m in self.metrics_history if m.val_seq_acc is not None]
        
        # Create figure with subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Metrics Dashboard', fontsize=16, fontweight='bold')
        
        # Plot 1: Loss
        ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2, alpha=0.8)
        if val_losses:
            ax1.plot(val_epochs, val_losses, 'r-', label='Val Loss', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training & Validation Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Token Accuracy
        ax2.plot(epochs, train_token_accs, 'b-', label='Train Token Acc', linewidth=2, alpha=0.8)
        if val_token_accs:
            ax2.plot(val_epochs, val_token_accs, 'r-', label='Val Token Acc', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Token Accuracy')
        ax2.set_title('Token-Level Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1)
        
        # Plot 3: Sequence Accuracy
        ax3.plot(epochs, train_seq_accs, 'b-', label='Train Seq Acc', linewidth=2, alpha=0.8)
        if val_seq_accs:
            ax3.plot(val_epochs, val_seq_accs, 'r-', label='Val Seq Acc', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Sequence Accuracy')
        ax3.set_title('Sequence-Level Accuracy')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
        
        # Plot 4: Learning Rate
        learning_rates = [m.learning_rate for m in self.metrics_history if m.learning_rate is not None]
        if learning_rates:
            lr_epochs = [m.epoch for m in self.metrics_history if m.learning_rate is not None]
            ax4.plot(lr_epochs, learning_rates, 'g-', linewidth=2, alpha=0.8)
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Learning Rate')
            ax4.set_title('Learning Rate Schedule')
            ax4.set_yscale('log')
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'No LR data', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Learning Rate Schedule')
        
        plt.tight_layout()
        
        # Save plot
        if save_path is None:
            save_path = os.path.join(self.save_dir, f"training_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Metrics plot saved to: {save_path}")
        
        # Also display
        plt.show()
        
    def save_metrics_json(self, filename: str = None):
        """Save all metrics to JSON file."""
        if filename is None:
            filename = os.path.join(self.save_dir, f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
        # Convert to serializable format
        metrics_data = []
        for m in self.metrics_history:
            metric_dict = {
                "epoch": m.epoch,
                "train_loss": m.train_loss,
                "train_token_acc": m.train_token_acc,
                "train_seq_acc": m.train_seq_acc,
                "val_loss": m.val_loss,
                "val_token_acc": m.val_token_acc,
                "val_seq_acc": m.val_seq_acc,
                "learning_rate": m.learning_rate,
                "timestamp": m.timestamp
            }
            metrics_data.append(metric_dict)
            
        with open(filename, 'w') as f:
            json.dump(metrics_data, f, indent=2)
        print(f"Metrics saved to: {filename}")
        
    def save_generated_examples(self, filename: str = None):
        """Save generated examples to JSON file."""
        if filename is None:
            filename = os.path.join(self.save_dir, f"generated_examples_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
        with open(filename, 'w') as f:
            json.dump(self.generated_examples_history, f, indent=2)
        print(f"Generated examples saved to: {filename}")


class GeneratedExampleSampler:
    """Smart sampling of generated examples during training."""
    
    def __init__(self, total_dataset_size: int, max_examples_per_epoch: int = 50):
        self.total_dataset_size = total_dataset_size
        self.max_examples_per_epoch = max_examples_per_epoch
        
        # For 4.5M samples, this gives reasonable coverage
        self.sampling_strategy = self._determine_sampling_strategy()
        
    def _determine_sampling_strategy(self):
        """Determine optimal sampling strategy based on dataset size."""
        if self.total_dataset_size < 10000:
            return "random_subset"  # Small dataset - random sampling
        elif self.total_dataset_size < 100000:
            return "stratified"     # Medium dataset - stratified sampling
        else:
            return "systematic"     # Large dataset - systematic sampling
            
    def sample_indices(self, dataset_size: int, epoch: int) -> List[int]:
        """Sample indices for generating examples."""
        if self.sampling_strategy == "random_subset":
            return random.sample(range(dataset_size), min(self.max_examples_per_epoch, dataset_size))
        
        elif self.sampling_strategy == "stratified":
            # Sample from different parts of the dataset
            n_samples = min(self.max_examples_per_epoch, dataset_size)
            indices = []
            for i in range(n_samples):
                # Sample from different strata
                stratum_idx = (i * dataset_size) // n_samples
                offset = random.randint(0, dataset_size // n_samples - 1)
                idx = min(stratum_idx + offset, dataset_size - 1)
                indices.append(idx)
            return indices
            
        else:  # systematic
            # Systematic sampling with random start, rotated by epoch
            step = max(1, dataset_size // self.max_examples_per_epoch)
            start = (epoch * 17) % step  # Use prime number to avoid patterns
            return list(range(start, dataset_size, step))[:self.max_examples_per_epoch]


def generate_examples_during_training_enhanced(model, dataset, tokenizer, 
                                             metrics_tracker: MetricsTracker,
                                             epoch: int, sampler: GeneratedExampleSampler,
                                             num_examples: int = 50):
    """Enhanced example generation with intelligent sampling and storage."""
    model.eval()
    device = next(model.parameters()).device
    
    # Get sample indices using smart sampling
    sample_indices = sampler.sample_indices(len(dataset), epoch)[:num_examples]
    
    generated_examples = []
    
    with torch.no_grad():
        for i, idx in enumerate(sample_indices):
            try:
                sample = dataset[idx]
                x_target = sample['x_target']
                
                # Decode the full target reaction
                original_reaction = tokenizer.decode(x_target.tolist())
                
                # Parse reaction to extract reactants and agents
                parts = original_reaction.split('>')
                if len(parts) >= 3:
                    reactants = parts[0]
                    agents = parts[1] if parts[1] else ""
                    true_product = parts[2]
                elif len(parts) == 2:
                    reactants = parts[0]
                    agents = ""
                    true_product = parts[1]
                else:
                    continue
                
                # Generate product with different temperatures for diversity
                temperatures = [0.8, 1.0, 1.2]
                generated_variants = []
                
                for temp in temperatures:
                    generated_product = model.generate_product(
                        reactants, agents, tokenizer, num_steps=50, temperature=temp
                    )
                    generated_variants.append({
                        "temperature": temp,
                        "product": generated_product.strip(),
                        "is_correct": generated_product.strip() == true_product.strip()
                    })
                
                # Store example
                example_data = {
                    "sample_idx": idx,
                    "reactants": reactants,
                    "agents": agents,
                    "true_product": true_product.strip(),
                    "generated_variants": generated_variants,
                    "input_format": f"{reactants}>{agents}>" if agents else f"{reactants}>>"
                }
                
                generated_examples.append(example_data)
                
                # Print first few examples for monitoring
                if i < 3:
                    reactants_agents_text = f"{reactants}>{agents}>" if agents else f"{reactants}>>"
                    best_match = max(generated_variants, key=lambda x: x["is_correct"])
                    match_indicator = "✓" if best_match["is_correct"] else "✗"
                    
                    print(f"      {i+1}. {reactants_agents_text}")
                    print(f"         True:      {true_product}")
                    print(f"         Generated: {best_match['product']} {match_indicator}")
                    if i < 2:
                        print()
                        
            except Exception as e:
                print(f"      Error generating example {i+1}: {str(e)}")
    
    # Add to metrics tracker
    metrics_tracker.add_generated_examples(epoch, generated_examples)
    
    # Calculate summary statistics
    correct_any_temp = sum(1 for ex in generated_examples 
                          if any(var["is_correct"] for var in ex["generated_variants"]))
    
    print(f"      Generated {len(generated_examples)} examples")
    print(f"      Correct products (any temp): {correct_any_temp}/{len(generated_examples)} ({100*correct_any_temp/max(len(generated_examples),1):.1f}%)")
    
    model.train()
    return generated_examples


def train_reaction_md4_enhanced(model, train_dataset, val_dataset, tokenizer,
                               num_epochs: int = 10, batch_size: int = 32,
                               learning_rate: float = 1e-4, validation_freq: int = 1,
                               save_dir: str = "training_logs",
                               examples_per_epoch: int = 50):
    """Enhanced training loop with comprehensive metrics tracking."""
    
    device = next(model.parameters()).device
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))
    
    # Initialize tracking
    metrics_tracker = MetricsTracker(save_dir)
    example_sampler = GeneratedExampleSampler(len(train_dataset), examples_per_epoch)
    
    best_val_loss = float('inf')
    
    print(f"Starting enhanced training for {num_epochs} epochs...")
    print(f"Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}")
    print(f"Saving metrics and examples to: {save_dir}")
    print(f"Examples per epoch: {examples_per_epoch}")
    print("=" * 80)
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_token_acc = 0
        train_seq_acc = 0
        num_successful_batches = 0
        
        pbar_train = tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch+1}/{num_epochs} [Training]",
            ncols=120,
            leave=True,
            position=0
        )
        
        for batch_idx, batch in enumerate(pbar_train):
            x_masked = batch['x_masked'].to(device)
            x_target = batch['x_target'].to(device) 
            product_mask = batch['product_mask'].to(device)
            
            batch_size_curr = x_masked.shape[0]
            
            # Sample random time steps
            t = torch.rand(batch_size_curr, device=device)
            
            # Forward diffusion (add noise to target based on schedule)
            x_noisy, is_masked = model.forward_sample_with_remasking(x_target, t, product_mask)

            # Forward pass
            outputs = model(x_noisy, t, x_target, product_mask)
            loss = outputs.get('loss', torch.tensor(0.0, device=device, requires_grad=True))
            
            # Skip if no valid loss
            if loss.item() <= 0 or not loss.requires_grad:
                continue
            
            # Calculate accuracy
            if product_mask.any():
                with torch.no_grad():
                    logits = outputs["logits"]
                    predictions = torch.argmax(logits, dim=-1)
                            
                    # Use the fixed accuracy calculation
                    acc_metrics = calculate_accuracy_fixed(
                        predictions, x_target, product_mask, 
                        model.config.pad_token_id, x_noisy, model.config.mask_token_id
                    )
                    train_token_acc += acc_metrics["token_accuracy"]
                    train_seq_acc += acc_metrics["sequence_accuracy"]
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            #train_loss += loss.item()
            train_loss += loss.item()
            num_successful_batches += 1
            
            # Update progress bar
            if num_successful_batches > 0:
                avg_train_loss = train_loss / num_successful_batches
                avg_token_acc = train_token_acc / num_successful_batches
                avg_seq_acc = train_seq_acc / num_successful_batches
                
                pbar_train.set_postfix({
                    "loss": f"{avg_train_loss:.4f}",
                    "tok_acc": f"{avg_token_acc:.3f}",
                    "seq_acc": f"{avg_seq_acc:.3f}",
                    "lr": f"{scheduler.get_last_lr()[0]:.2e}"
                })
        
        pbar_train.close()
        
        # Calculate final training metrics
        if num_successful_batches > 0:
            avg_train_loss = train_loss / num_successful_batches
            avg_train_token_acc = train_token_acc / num_successful_batches
            avg_train_seq_acc = train_seq_acc / num_successful_batches
        else:
            avg_train_loss = avg_train_token_acc = avg_train_seq_acc = 0.0
        
        # Create metrics object
        epoch_metrics = TrainingMetrics(
            epoch=epoch + 1,
            train_loss=avg_train_loss,
            train_token_acc=avg_train_token_acc,
            train_seq_acc=avg_train_seq_acc,
            learning_rate=scheduler.get_last_lr()[0]
        )
        
        # Validation phase
        if (epoch + 1) % validation_freq == 0:
            model.eval()
            val_loss = 0
            val_token_acc = 0
            val_seq_acc = 0
            num_val_batches = 0
            
            pbar_val = tqdm(
                val_dataloader, 
                desc=f"Epoch {epoch+1}/{num_epochs} [Validation]",
                ncols=120,
                leave=True,
                position=0
            )
            
            with torch.no_grad():
                for batch in pbar_val:
                    x_masked = batch['x_masked'].to(device)
                    x_target = batch['x_target'].to(device)
                    product_mask = batch['product_mask'].to(device)
                    
                    batch_size_curr = x_masked.shape[0]
                    
                    # Sample random time steps for validation
                    t = torch.rand(batch_size_curr, device=device)
                    
                    # Forward diffusion
                    x_noisy, is_masked = model.forward_sample_with_remasking(x_target, t, product_mask)
                    
                    # Forward pass
                    outputs = model(x_noisy, t, x_target, product_mask)
                    loss = outputs.get('loss', torch.tensor(0.0, device=device))
                    
                    # Calculate accuracy
                    if product_mask.any():
                        with torch.no_grad():
                            logits = outputs["logits"]
                            predictions = torch.argmax(logits, dim=-1)
                            
                            # Use the fixed accuracy calculation
                            acc_metrics = calculate_accuracy_fixed(
                                predictions, x_target, product_mask, 
                                model.config.pad_token_id, x_noisy, model.config.mask_token_id
                            )
                            val_token_acc += acc_metrics["token_accuracy"]
                            val_seq_acc += acc_metrics["sequence_accuracy"]
                    
                    #val_loss += loss.item()
                    val_loss += loss.item()
                    num_val_batches += 1
                    
                    # Update validation progress bar
                    if num_val_batches > 0:
                        avg_val_loss = val_loss / num_val_batches
                        avg_val_token_acc = val_token_acc / num_val_batches
                        avg_val_seq_acc = val_seq_acc / num_val_batches
                        
                        pbar_val.set_postfix({
                            "loss": f"{avg_val_loss:.4f}",
                            "tok_acc": f"{avg_val_token_acc:.3f}",
                            "seq_acc": f"{avg_val_seq_acc:.3f}"
                        })
            
            pbar_val.close()
            
            # Update metrics with validation results
            if num_val_batches > 0:
                epoch_metrics.val_loss = val_loss / num_val_batches
                epoch_metrics.val_token_acc = val_token_acc / num_val_batches
                epoch_metrics.val_seq_acc = val_seq_acc / num_val_batches
            
            # Print epoch summary
            print(f"\n EPOCH {epoch+1} SUMMARY:")
            print(f"   Train → Loss: {avg_train_loss:.4f} | Token Acc: {avg_train_token_acc:.3f} | Seq Acc: {avg_train_seq_acc:.3f}")
            print(f"   Val   → Loss: {epoch_metrics.val_loss:.4f} | Token Acc: {epoch_metrics.val_token_acc:.3f} | Seq Acc: {epoch_metrics.val_seq_acc:.3f}")
            print(f"   LR: {scheduler.get_last_lr()[0]:.2e}")
            
            # Check for best model
            if epoch_metrics.val_loss < best_val_loss:
                best_val_loss = epoch_metrics.val_loss
                print(f"    🎯 NEW BEST VALIDATION LOSS! ({epoch_metrics.val_loss:.4f})")
                
                # Save best model
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'config': model.config,
                    'val_loss': best_val_loss,
                    'metrics_history': metrics_tracker.metrics_history
                }, os.path.join(save_dir, 'best_model.pt'))
            
            # Generate examples and store them
            print(f"\n    📊 GENERATED EXAMPLES:")
            generate_examples_during_training_enhanced(
                model, val_dataset, tokenizer, metrics_tracker, epoch + 1, 
                example_sampler, examples_per_epoch
            )
            
        else:
            print(f"\n EPOCH {epoch+1} SUMMARY:")
            print(f"   Train → Loss: {avg_train_loss:.4f} | Token Acc: {avg_train_token_acc:.3f} | Seq Acc: {avg_train_seq_acc:.3f}")
            print(f"   LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # Add metrics to tracker
        metrics_tracker.add_metrics(epoch_metrics)
        
        # Save metrics periodically
        if (epoch + 1) % 5 == 0:
            metrics_tracker.save_metrics_json()
            metrics_tracker.save_generated_examples()
            
        # Plot metrics periodically
        if (epoch + 1) % 5 == 0:
            metrics_tracker.plot_metrics()
            
        print("=" * 80)
    
    print(f"\n🎉 TRAINING COMPLETED!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    
    # Final saves
    metrics_tracker.save_metrics_json()
    metrics_tracker.save_generated_examples()
    
    # Final comprehensive plot
    print("\n📈 Creating final metrics visualization...")
    metrics_tracker.plot_metrics(os.path.join(save_dir, "final_training_metrics.png"))
    
    return metrics_tracker


# Example usage and main training script
if __name__ == "__main__":
    
    # Load your data
    print("Loading reaction data...")
    with open("/home/gpwuq/ipms-foundation_model/data/interim/clean_corpus_only_reactions.txt") as f:
        reactions = [line.strip() for line in f if line.strip()]
    
    print(f"Loaded {len(reactions)} reactions")
    
    # Initialize tokenizer
    tokenizer = CustomTokenizer("/home/gpwuq/ipms-foundation_model/data/interim/clean_vocab_only_reactions_smiles_combined.json")
    
    # Configuration
    config = ReactionMD4Config(
        vocab_size=len(tokenizer.vocab),
        max_seq_length=230,
        d_model=512,
        n_layers=6,
        n_heads=8,
        d_ff=2048,
        dropout=0.1,
        max_product_tokens=82,
        reactant_sep_token_id=23,
        pad_token_id=0,
        eos_token_id=tokenizer.eos_token_id,
        noagent_token_id=tokenizer.noagent_token_id,
        noise_schedule="cosine",
        timesteps=1000
    )
    
    print(f"\nUsing configuration:")
    print(f"  max_seq_length: {config.max_seq_length}")
    print(f"  max_product_tokens: {config.max_product_tokens}")
    print(f"  vocab_size: {config.vocab_size}")
    print(f"  noise_schedule: {config.noise_schedule}")
    
    # Shuffle and split data
    random.shuffle(reactions)
    
    train_size = int(0.8 * len(reactions))
    val_size = int(0.1 * len(reactions))
    
    train_data = reactions[:train_size]
    val_data = reactions[train_size:train_size+val_size]
    test_data = reactions[train_size+val_size:]
    
    print(f"\nData split:")
    print(f"  Train: {len(train_data):,} samples")
    print(f"  Val: {len(val_data):,} samples") 
    print(f"  Test: {len(test_data):,} samples")
    
    # Initialize model
    device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    model = ReactionMD4Remasking(config).to(device)
    
    # Create datasets
    train_dataset = ReactionDataset(train_data, tokenizer, config)
    val_dataset = ReactionDataset(val_data, tokenizer, config)
    test_dataset = ReactionDataset(test_data, tokenizer, config)
    
    examples_per_epoch = 75  # Good for 4.5M dataset
    num_epochs = 10
    
    print(f"\nTraining configuration:")
    print(f"  Examples per epoch: {examples_per_epoch}")
    print(f"  Total examples over training: ~{examples_per_epoch * num_epochs}")
    print(f"  Storage strategy: Systematic sampling with rotation")

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nModel initialized:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Model size: ~{total_params * 4 / 1024**2:.1f} MB")
    
    # Training
    print(f"\nStarting training...")
    
    # Enhanced training with metrics tracking
    metrics_tracker = train_reaction_md4_enhanced(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        tokenizer=tokenizer,
        num_epochs=10,
        batch_size=32,
        learning_rate=1e-4,
        validation_freq=1,
        save_dir="training_logs",
        examples_per_epoch=examples_per_epoch
    )
    
    print("\n📊 Training complete! Check 'training_logs' directory for:")
    print("  - training_metrics_*.png: Comprehensive plots")
    print("  - metrics_*.json: Raw metrics data")
    print("  - generated_examples_*.json: Generated examples")
    print("  - best_model.pt: Best model checkpoint")
    
    # Final test evaluation
    print("\n" + "="*80)
    print("RUNNING FINAL TEST EVALUATION")
    print("="*80)
    
    test_results = test_model(
        model=model,
        test_dataset=test_dataset,
        tokenizer=tokenizer,
        batch_size=32,
        num_steps=100
    )
    
    print(f"\n TRAINING COMPLETED!")
    print(f"Final test token accuracy: {test_results['token_accuracy']:.4f}")
    print(f"Final test sequence accuracy: {test_results['sequence_accuracy']:.4f}")
    
    # Example of generating a product for a new reaction
    print("\n" + "="*50)
    print("EXAMPLE GENERATION")
    print("="*50)
    
    # Test generation with a simple example
    reactants = "CCO"
    agents = "NaOH"  # or "" for no agents
    
    print(f"Input: {reactants}>{agents}>")
    generated_product = model.generate_product(reactants, agents, tokenizer, num_steps=100)
    print(f"Generated product: {generated_product}")
    
    # Optional: Save the trained model
    # torch.save({
    #     'model_state_dict': model.state_dict(),
    #     'config': config,
    #     'test_results': test_results
    # }, 'reaction_md4.pt')
    # print("Model saved as 'reaction_md4.pt'")

    metrics_tracker.plot_metrics(save_path="final_metrics.png")
    metrics_tracker.plot_individual_metrics(save_dir="individual_plots")
