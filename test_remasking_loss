import torch
import torch.nn.functional as F
import numpy as np
from dataclasses import dataclass
import sys
sys.path.append(".")
from loss_function.loss_implementations import ReMDMELBOLoss



@dataclass
class TestConfig:
    timesteps: int = 50
    mask_token_id: int = 103
    pad_token_id: int = 0
    noise_schedule: str = "cosine"
    vocab_size: int = 500
    batch_size: int = 4
    seq_len: int = 16
    base_sigma: float = 0.5
    temperature: float = 1.0


class MockModel(torch.nn.Module):
    def __init__(self, vocab_size: int):
        super().__init__()
        self.vocab_size = vocab_size

    def forward(self, x_noisy, t, x_target=None, return_perfect=False, return_random=False):
        batch_size, seq_len = x_noisy.shape
        if return_perfect:
            if x_target is None:
                raise ValueError("x_target required for perfect predictions")
            logits = torch.full((batch_size, seq_len, self.vocab_size), -10.0)
            for b in range(batch_size):
                for s in range(seq_len):
                    target_class = x_target[b, s].item()
                    if target_class != 0:  # skip pad
                        logits[b, s, target_class] = 10.0
            return {"logits": logits}
        elif return_random:
            return {"logits": torch.randn(batch_size, seq_len, self.vocab_size)}
        else:
            return {"logits": torch.randn(batch_size, seq_len, self.vocab_size, requires_grad=True)}


class TestReMDMLoss:
    def __init__(self):
        self.config = TestConfig()
        self.loss_fn = ReMDMELBOLoss(self.config)
        self.model = MockModel(self.config.vocab_size)

    def test_mathematical_properties(self):
        print("\n=== TEST 1: MATHEMATICAL PROPERTIES ===")
        x_target = torch.randint(1, self.config.vocab_size, 
                                 (self.config.batch_size, self.config.seq_len))
        product_mask = torch.ones_like(x_target, dtype=torch.bool)

        result = self.loss_fn.compute_elbo_loss(
            model=self.model,
            x_target=x_target,
            product_mask=product_mask
        )
        loss = result["loss"]

        print("Loss value:", loss.item())
        assert loss.item() >= 0, "Loss must be non-negative"
        assert torch.isfinite(loss), "Loss must be finite"
        assert loss.dim() == 0, "Loss must be a scalar"
        print("‚úì Passed mathematical checks")

    def test_gradient_flow(self):
        print("\n=== TEST 2: GRADIENT FLOW ===")
        model = torch.nn.Linear(self.config.seq_len, self.config.vocab_size)

        def model_forward(x_noisy, t):
            x_embed = F.one_hot(x_noisy, num_classes=self.config.vocab_size).float()
            x_flat = x_embed.view(x_noisy.size(0), -1)
            logits_flat = model(x_flat[:, :self.config.seq_len])
            logits = logits_flat.unsqueeze(1).expand(-1, x_noisy.size(1), -1)
            return {"logits": logits}

        x_target = torch.randint(1, self.config.vocab_size, 
                                 (self.config.batch_size, self.config.seq_len))
        product_mask = torch.ones_like(x_target, dtype=torch.bool)

        result = self.loss_fn.compute_elbo_loss(
            model=model_forward,
            x_target=x_target,
            product_mask=product_mask
        )
        loss = result["loss"]
        loss.backward()

        grads_ok = any(param.grad is not None for param in model.parameters())
        assert grads_ok, "No gradients propagated"
        print("‚úì Gradients flow through the loss")

    def test_behavioral_properties(self):
        print("\n=== TEST 3: BEHAVIORAL PROPERTIES ===")
        x_target = torch.randint(1, self.config.vocab_size, 
                                 (self.config.batch_size, self.config.seq_len))
        product_mask = torch.ones_like(x_target, dtype=torch.bool)

        # Random predictions
        result_random = self.loss_fn.compute_elbo_loss(
            model=lambda x, t: self.model(x, t, return_random=True),
            x_target=x_target,
            product_mask=product_mask
        )
        print("Random prediction loss:", result_random["loss"].item())

        # Perfect predictions
        result_perfect = self.loss_fn.compute_elbo_loss(
            model=lambda x, t: self.model(x, t, x_target=x_target, return_perfect=True),
            x_target=x_target,
            product_mask=product_mask
        )
        print("Perfect prediction loss:", result_perfect["loss"].item())

        assert result_perfect["loss"].item() <= result_random["loss"].item(), \
            "Perfect predictions should not have higher loss than random"
        print("‚úì Loss behaves correctly (perfect ‚â§ random)")

    def test_remasking_behavior(self):
        print("\n=== TEST 4: REMASKING BEHAVIOR ===")
        x_target = torch.randint(1, self.config.vocab_size, 
                                 (self.config.batch_size, self.config.seq_len))
        product_mask = torch.ones_like(x_target, dtype=torch.bool)

        result = self.loss_fn.compute_elbo_loss(
            model=self.model,
            x_target=x_target,
            product_mask=product_mask
        )
        print("Masked tokens:", result["num_masked_tokens"])
        print("Remasked tokens:", result["num_remasked_tokens"])
        print("Mean sigma:", result["mean_sigma"])
        assert result["num_masked_tokens"] >= 0
        assert result["num_remasked_tokens"] >= 0
        print("‚úì Remasking stats collected")

    def test_edge_cases(self):
        print("\n=== TEST 5: EDGE CASES ===")
        # all padding
        x_target = torch.full((self.config.batch_size, self.config.seq_len),
                              self.config.pad_token_id)
        product_mask = torch.ones_like(x_target, dtype=torch.bool)
        result = self.loss_fn.compute_elbo_loss(
            model=self.model,
            x_target=x_target,
            product_mask=product_mask
        )
        print("All pad loss:", result["loss"].item())
        assert result["loss"].item() == 0.0, "All padding should yield 0 loss"

        # empty mask
        x_target = torch.randint(1, self.config.vocab_size,
                                 (self.config.batch_size, self.config.seq_len))
        product_mask = torch.zeros_like(x_target, dtype=torch.bool)
        result = self.loss_fn.compute_elbo_loss(
            model=self.model,
            x_target=x_target,
            product_mask=product_mask
        )
        print("Empty mask loss:", result["loss"].item())
        assert result["loss"].item() == 0.0, "Empty mask should yield 0 loss"
        print("‚úì Edge cases handled")

    def run_all_tests(self):
        tests = [
            self.test_mathematical_properties,
            self.test_gradient_flow,
            self.test_behavioral_properties,
            self.test_remasking_behavior,
            self.test_edge_cases,
        ]
        all_ok = True
        for test in tests:
            try:
                test()
            except Exception as e:
                print(f"‚ùå {test.__name__} failed: {e}")
                all_ok = False
        if all_ok:
            print("\nüéâ All ReMDM tests passed!")
        else:
            print("\n‚ö† Some ReMDM tests failed.")


def main():
    tester = TestReMDMLoss()
    tester.run_all_tests()


if __name__ == "__main__":
    main()
