import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import sys
import random
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import numpy as np
from tqdm.auto import tqdm
from torch.utils.data import Dataset, DataLoader
import json
import matplotlib.pyplot as plt
from datetime import datetime
import os
sys.path.append(".")
from discrete_diffusion_language_models.src.tokenizer import CustomTokenizer
from loss_function.loss_implementations import ELBOLoss

@dataclass
class ReactionMD4Config:
    """Configuration for Reaction Product Generation MD4."""
    vocab_size: int = 1447  
    max_seq_length: int = 350 # 230 before 
    d_model: int = 512
    n_layers: int = 6
    n_heads: int = 8
    d_ff: int = 2048
    dropout: float = 0.1
    mask_token_id: int = 2
    reactant_sep_token_id: int = 25
    pad_token_id: int = 0
    eos_token_id: int = 3
    noagent_token_id: int = 5
    max_product_tokens: int = 200 # 82 before
    noise_schedule: str = "cosine"  # "linear", "cosine", "geometric"
    timesteps: int = 1000
    use_elbo_loss: bool = True  # Flag to use ELBO loss
    
    def __post_init__(self):
        if self.mask_token_id == -1:
            self.mask_token_id = self.vocab_size


def preprocess_reaction_for_training(reaction_str: str, tokenizer, config: ReactionMD4Config):
    """
    Preprocess reaction for training - masks only the product part.
    
    Args:
        reaction_str: e.g., 'CCO>NaOH>CCCBr' or 'CCO>>CCCBr'
        tokenizer: CustomTokenizer instance
        config: Model configuration
    
    Returns:
        x_masked: reaction with product tokens replaced by MASK
        x_target: original reaction sequence  
        product_mask: boolean mask for product positions
    """

    # Split the SMILES reaction into parts by > seperators
    parts = reaction_str.split(">")
    
    # Two supported formats:
    # Either CCO>NaOH>CCCBr or CCO>>CCCBr
    # If the reaction has no agent present (CCO>>CCCBr) choose agent as [NOAGENT]
    
    if len(parts) == 3 and parts[1] == "":
        reactants, product = parts[0], parts[2]
        agents = tokenizer.noagent_token
    elif len(parts) == 3:
        reactants, agents, product = parts
    elif len(parts) == 2:
        reactants, product = parts
        agents = tokenizer.noagent_token
    else:
        raise ValueError(f"Unexpected reaction format: {reaction_str}")
    
    # Tokenize each part
    react_ids = tokenizer.encode(reactants)
    agents_ids = tokenizer.encode(agents)
    prod_ids = tokenizer.encode(product)
    
    # Add separators and EOS
    # Build the target sequence as: reactants + '>' + agents + '>' + product + EOS
    sep_id = tokenizer.get_special_token_id(">")
    x_target = (
        react_ids + [sep_id] +
        agents_ids + [sep_id] +
        prod_ids + [tokenizer.eos_token_id]
    )
    
    # Build the input sequence for training:
    # Create masked version - replace product tokens with MASK and append EOS
    x_masked = (
        react_ids + [sep_id] +
        agents_ids + [sep_id] +
        [config.mask_token_id] * len(prod_ids) + 
        [tokenizer.eos_token_id]
    )
    
    # Hard truncate both sequences to max_seq_length
    if len(x_target) > config.max_seq_length:
        x_target = x_target[:config.max_seq_length]
        x_masked = x_masked[:config.max_seq_length]
    
    # Right-pad both to exactly max_seq_length with the padding token [PAD]
    pad_len = config.max_seq_length - len(x_target)
    x_target += [config.pad_token_id] * pad_len
    x_masked += [config.pad_token_id] * pad_len
    
    # Product mask: True for product positions (after second '>')
    #Initialize a boolean mask of length max_seq_length, all False
    product_mask = [False] * config.max_seq_length
    # Compute the index where the first product token would sit in, so position after second '>' before padding
    second_sep_idx = len(react_ids) + 1 + len(agents_ids) + 1  
    
    # Mark positions corresponding to product tokens as True, but do not exceed max_seq_length
    for i in range(second_sep_idx, min(second_sep_idx + len(prod_ids), config.max_seq_length)):
        product_mask[i] = True
    
    # x_masked: shape (max_seq_length,) -> product tokens replaced by mask_token_id
    # x_target: shape (max_seq_length,) -> the full ground-truth sequence
    # product_mask: shape (max_seq_length,) -> True where product tokens are (before padding/truncation), False everywhere else
    return torch.tensor(x_masked), torch.tensor(x_target), torch.tensor(product_mask)


def create_inference_input(reactants: str, agents: str, tokenizer, config: ReactionMD4Config):
    """
    Create input for inference: reactants>agents>[MASK]...[MASK]
    """
    # If there are no agents supplied, replace it with the [NOAGENT] token
    if not agents:
        agents = tokenizer.noagent_token
    
    # Tokenize the reactant and agent
    react_ids = tokenizer.encode(reactants)
    agents_ids = tokenizer.encode(agents)
    sep_id = tokenizer.get_special_token_id(">")
    
    # Create prefix sequence that will always be visible: reactants + '>' + agents + '>'
    prefix = react_ids + [sep_id] + agents_ids + [sep_id]
    
    # Ensures the prefix doesnt already take up almost the entire max sequence length
    # Check if prefix is too long - leave space for at least 1 mask token + EOS
    min_required_space = 2  # 1 mask token + 1 EOS token
    if len(prefix) >= config.max_seq_length - min_required_space:
        # Option 1: Truncate the prefix to fit
        max_prefix_len = config.max_seq_length - min_required_space
        prefix = prefix[:max_prefix_len]
        print(f"Warning: Prefix truncated from {len(react_ids + [sep_id] + agents_ids + [sep_id])} to {len(prefix)} tokens")
    
    # Use for decoding later: The index where the first product token should appear
    product_start_idx = len(prefix)
    
    # Fill rest with MASK tokens until max_len - 1 (reserve space for EOS)
    n_mask_tokens = config.max_seq_length - len(prefix) - 1  # -1 for EOS
    
    # Ensure we have at least 1 mask token
    if n_mask_tokens <= 0:
        raise ValueError(f"Cannot fit sequence: prefix_len={len(prefix)}, max_seq_length={config.max_seq_length}")
    
    # Build final sequence
    input_seq = prefix + [config.mask_token_id] * n_mask_tokens + [tokenizer.eos_token_id]
    
    # Pad if needed
    if len(input_seq) < config.max_seq_length:
        input_seq += [config.pad_token_id] * (config.max_seq_length - len(input_seq))
    
    return torch.tensor(input_seq), product_start_idx


class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding. Used to inject sequence order information into token embeddings for the transformer backbone"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        # Create a tensor pe of shape (max_len, d_model) to hold all positional encodings
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].unsqueeze(0)


class TimeEmbedding(nn.Module):
    """Time embedding for diffusion timestep."""
    
    # Defines a small MLP that will process the sinusoidal embedding
    # lets the model learn a richer representation of timesteps
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model)
        )
        
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        # Sinusoidal embedding
        # Each timestep embedding will use half_dim sine and half_dim cosine components
        half_dim = self.d_model // 2
        # Compute frequency scaling factor
        emb = math.log(10000) / (half_dim - 1)
        # Frequencies decreasing exponentially
        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb) # shape: (half_dim,)
        # Multiply each timestep t[i] with the frequencies
        # each timestep is expanded into sinusoidal arguments
        emb = t[:, None] * emb[None, :] # shape: (batch, half_dim)
        # Concatenate sine and cosine parts
        emb = torch.cat([emb.sin(), emb.cos()], dim=-1) # shape: (batch, half_dim)
        
        return self.mlp(emb)
    
class TransformerBlockNoTime(nn.Module):
    """Transformer block without time conditioning."""
    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.d_model)
        self.attn = nn.MultiheadAttention(
            config.d_model, config.n_heads, dropout=config.dropout,
            batch_first=True
        )
        self.ln2 = nn.LayerNorm(config.d_model)
        self.mlp = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_model),
            nn.Dropout(config.dropout)
        )
    
    def forward(self, x):  # No time_emb parameter
        # Self-attention
        x_norm = self.ln1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        x = x + attn_out
        
        # MLP without time conditioning
        x_norm = self.ln2(x)
        mlp_out = self.mlp(x_norm)
        x = x + mlp_out
        
        return x


class TransformerBlock(nn.Module):
    """Transformer (encoder-only) block with time conditioning."""
    
    def __init__(self, config: ReactionMD4Config):
        super().__init__()
        self.config = config
        
        self.ln1 = nn.LayerNorm(config.d_model)
        self.attn = nn.MultiheadAttention(
            config.d_model, config.n_heads, dropout=config.dropout,
            batch_first=True
        )
        self.ln2 = nn.LayerNorm(config.d_model)
        
        # Time-conditioned MLP
        self.time_mlp = nn.Linear(config.d_model, config.d_model)
        self.mlp = nn.Sequential(
            nn.Linear(config.d_model, config.d_ff),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_model),
            nn.Dropout(config.dropout)
        )
        
    def forward(self, x: torch.Tensor, time_emb: torch.Tensor, 
                attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention + Layer Norm
        x_norm = self.ln1(x) # (batch, seq_len, d_model)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=attn_mask) # MHA output (batch, seq_len, d_model)
        x = x + attn_out # Residual add (batch, seq_len, d_model)
        
        # Time-conditioned MLP + Layer Norm
        x_norm = self.ln2(x) # (batch, seq_len, d_model)
        time_cond = self.time_mlp(time_emb).unsqueeze(1)  # (batch, 1, d_model)
        x_norm = x_norm + time_cond  # Broadcast time conditioning (batch, seq_len, d_model)

        # Feedforward MLP + Residual
        mlp_out = self.mlp(x_norm) # (batch, seq_len, d_model)
        x = x + mlp_out # (batch, seq_len, d_model)
        
        return x


class ReactionMD4(nn.Module):
    """
    Reaction Product Generation using Masked Diffusion (MD4).
    Only masks and generates the product part of reactions.
    """
    
    def __init__(self, config: ReactionMD4Config):
        super().__init__()
        self.config = config
        
        # Token embeddings (including mask token)
        # Each token ID is mapped to a d_model-dimensional embedding (No need for +1 because mask_token_id is included in the embedding table)
        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)
        
        # Position encoding
        # Add sinusoidal positional encodings to token embeddings
        self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_length * 2)
        
        # Time embedding
        # Converts diffusion timestep t into a d_model-dimensional embedding
        #self.time_embedding = TimeEmbedding(config.d_model)
        
        # Transformer backbone
        """ self.layers = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ]) """

        self.layers = nn.ModuleList([
            TransformerBlockNoTime(config) for _ in range(config.n_layers)
        ])
        
        self.ln_f = nn.LayerNorm(config.d_model)
        
        # Output head -> Linear layer to map hidden states to vocabulary logits + Dropout for Regularization
        self.output_head = nn.Linear(config.d_model, config.vocab_size)
        self.dropout = nn.Dropout(config.dropout)
        
        # Initialize ELBO loss if enabled
        if config.use_elbo_loss:
            self.elbo_loss = ELBOLoss(config)
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """Initialize weights following standard transformer initialization."""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def get_masking_schedule(self, t: torch.Tensor) -> torch.Tensor:
        """Get masking schedule alpha_t based on configuration."""
        if self.config.noise_schedule == "linear":
            return 1 - t
        elif self.config.noise_schedule == "cosine":
            return torch.cos(torch.pi / 2 * t)
        elif self.config.noise_schedule == "geometric":
            beta_min, beta_max = 1e-5, 20
            return torch.exp(-beta_min * (1-t) * beta_max**t)
        else:
            raise ValueError(f"Unknown schedule: {self.config.noise_schedule}")
    
    def forward_sample(self, x_target: torch.Tensor, t: torch.Tensor, product_mask: torch.Tensor) -> torch.Tensor:
        """
        Forward diffusion process - only mask tokens in product region.
        
        Args:
            x_target: target sequence [batch, seq_len]
            t: time step [batch]
            product_mask: [batch, seq_len] boolean mask for product positions
        """
        alpha_t = self.get_masking_schedule(t)  # [batch]
        alpha_t = alpha_t.unsqueeze(-1)  # [batch, 1]
        
        # Sample which positions to keep unmasked
        keep_prob = torch.rand_like(x_target, dtype=torch.float)
        # get indices of tokens that we keep
        should_keep = keep_prob < alpha_t
        
        # Only apply masking to product positions
        # Randomly masks tokens according to alpha_t -> creates x_noisy
        should_mask = product_mask & ~should_keep
        # if should_mask[i,j] == True, the token is replaced with the mask token
        x_noisy = torch.where(should_mask, self.config.mask_token_id, x_target)
        return x_noisy
    
    # Standard transformer forward pass with time conditioning
    def forward(self, input_ids: torch.Tensor, t: torch.Tensor,
                target_ids: Optional[torch.Tensor] = None,
                product_mask: Optional[torch.Tensor] = None,
                use_elbo_loss: bool = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass of the model.
        
        Args:
            input_ids: Input token IDs [batch, seq_len]
            t: Diffusion time steps [batch] 
            target_ids: Target token IDs for training [batch, seq_len]
            product_mask: Boolean mask for product positions [batch, seq_len]
            use_elbo_loss: Whether to use ELBO loss (overrides config if provided)
        """
        batch_size, seq_len = input_ids.shape
        
        # Decide whether to use ELBO loss
        use_elbo = use_elbo_loss if use_elbo_loss is not None else self.config.use_elbo_loss

        # Token embeddings
        token_emb = self.token_embedding(input_ids)  # [batch, seq_len, d_model]
        
        # Add positional encoding
        x = self.pos_encoding(token_emb)
        x = self.dropout(x)
        
        # Time embedding
        # time_emb = self.time_embedding(t)  # [batch, d_model]
        
        # Apply transformer layers and map to vocab logits
        """ for layer in self.layers:
            x = layer(x, time_emb) """

        # Apply transformer layers without time
        for layer in self.layers:
            x = layer(x)  # No time_emb argument
        
        x = self.ln_f(x)
        
        # Output logits (only for vocabulary, not mask token)
        logits = self.output_head(x)  # [batch, seq_len, vocab_size]
        
        outputs = {"logits": logits}
        
        # Compute loss during training
        if target_ids is not None and product_mask is not None:
            if use_elbo and hasattr(self, 'elbo_loss'):
                # Use ELBO loss
                elbo_results = self.elbo_loss.compute_elbo_loss(
                    model=self,
                    x_target=target_ids,
                    product_mask=product_mask,
                    use_importance_sampling=True,
                    precomputed_logits=logits,
                    precomputed_t=t,
                    precomputed_x_noisy=input_ids
                )
                outputs["loss"] = elbo_results["loss"]
                outputs["elbo_metrics"] = {
                    "num_masked_tokens": elbo_results["num_masked_tokens"],
                    "mean_importance_weight": elbo_results["mean_importance_weight"],
                    "t_mean": elbo_results["t_mean"]
                }
            else:
                # Use original loss (fallback)
                # Check if there are any valid positions
                if product_mask.any():
                    # Cross-entropy loss only on product positions by creating a mask for valid positions
                    # True for product tokens and positions that are not padding
                    # Create mask for positions that are currently masked AND in product region
                    is_masked = (input_ids == self.config.mask_token_id)
                    valid_positions = product_mask & (target_ids != self.config.pad_token_id) & is_masked
                    
                    # check if there are any valid positions left
                    if valid_positions.any():
                        # Get the relevant tokens for calculating the CE
                        valid_targets = target_ids[valid_positions]
                        valid_logits = logits[valid_positions]
                        
                        # Compute CE loss and store it 
                        loss = F.cross_entropy(valid_logits, valid_targets, reduction='mean')
                        outputs["loss"] = loss
                    else:
                        # For no product tokens return 0
                        outputs["loss"] = torch.tensor(0.0, device=input_ids.device, requires_grad=True)
                else:
                    outputs["loss"] = torch.tensor(0.0, device=input_ids.device, requires_grad=True)
        
        # return dict containing the loss and the logits
        return outputs
    
    def sample_step(self, xt: torch.Tensor, t_curr: torch.Tensor, t_next: torch.Tensor,
                   product_mask: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
        """
        Single sampling step - only update masked positions in product region.
        (One diffusion step (update some masked tokens))
        """
        # The model predicts logits for each masked token in xt at timestep t_curr
        # Dividing by temperature controls sharpness:
        # <1 sharper, more greedy
        # >1 softer, more random
        outputs = self(xt, t_curr)
        logits = outputs["logits"] / temperature
        
        # Calculate transition probabilities
        # alpha_t: probability of being already unmasked at current time t_curr
        # alpha_s: probability of being unmasked at the next step t_next
        alpha_t = self.get_masking_schedule(t_curr).unsqueeze(-1)  # [batch, 1]
        alpha_s = self.get_masking_schedule(t_next).unsqueeze(-1)  # [batch, 1]
        
        # Fraction is the conditional probability that a masked token at time t gets unmasked at t_next
        # The difference (alpha_s - alpha_t) tells us how much more of the sequence should be revealed between timesteps
        p_unmask = (alpha_s - alpha_t) / (1 - alpha_t + 1e-8)
        p_unmask = torch.clamp(p_unmask, 0.0, 1.0) # keep prob between 0 and 1
        
        # Sample which positions to unmask
        is_mask = (xt == self.config.mask_token_id) # boolean tensor -> where tokens are still [MASK]
        # product_mask: boolean tensor -> which tokens are product region
        can_unmask = is_mask & product_mask # only masked product tokens can be updated
        
        if can_unmask.any():
            # Decide which positions to unmask this step
            # A token is selected for unsmaking if:
            # It is currently masked
            # It is inside the product region
            # Its random sample falls below p_unmask
            unmask_probs = torch.rand_like(xt, dtype=torch.float)
            positions_to_unmask = can_unmask & (unmask_probs < p_unmask)
            
            # Predict replacements for unmasked positions
            if positions_to_unmask.any():
                # Sample new tokens for unmasked positions
                probs = F.softmax(logits[positions_to_unmask], dim=-1)
                # sampled_tokens = torch.multinomial(probs, 1).squeeze(-1) # sampling
                sampled_tokens = torch.argmax(probs, dim=-1) # greedy insstead of sampling
                xt = xt.clone()
                xt[positions_to_unmask] = sampled_tokens
        
        return xt
    
    @torch.no_grad()
    def sample(self, reactants_list: List[str], agents_list: List[str], tokenizer,
               num_steps: int = 100, temperature: float = 1.0) -> List[torch.Tensor]:
        """
        This function generates products (SMILES) from reactants + agents using reverse diffusion 
        by repeatedly calling sample_step function (entire diffusion trajectory)
        
        Args:
            reactants_list: List of reactant SMILES strings
            agents_list: List of agent SMILES strings (can be empty strings)
            tokenizer: CustomTokenizer instance
            num_steps: Number of diffusion steps
            temperature: Sampling temperature
        """

        # If no input is provided, return an empty list, otherwise continue with batch generation
        batch_size = len(reactants_list)
        if batch_size == 0:
            return []
            
        device = next(self.parameters()).device
        
        # Create initial sequences with masked product regions
        # Builds a product_mask, so True only in the product region but excludes [EOS]
        initial_seqs = []
        product_masks = []
        product_starts = []
        
        for reactants, agents in zip(reactants_list, agents_list):
            input_seq, product_start = create_inference_input(
                reactants, agents, tokenizer, self.config
            )
            initial_seqs.append(input_seq)
            product_starts.append(product_start)
            
            # Create product mask
            # identifies which tokens should be generated (unmasked) during sampling
            product_mask = torch.zeros(self.config.max_seq_length, dtype=torch.bool)
            # Mark product region (excluding EOS position)
            product_mask[product_start:-1] = True
            product_masks.append(product_mask)
        
        # Batch the data
        xt = torch.stack(initial_seqs).to(device)  # [batch, seq_len]
        product_mask = torch.stack(product_masks).to(device)  # [batch, seq_len]
        
        # Create sampling decreasing schedule
        timesteps = torch.linspace(1.0, 0.0, num_steps + 1, device=device)
        
        # Reverse diffusion process
        # Iteratively applies sample_step num_steps times:
        # 1. Model predicts logits for masked product positions.
        # 2. Decide which positions to unmask according to p_unmask.
        # 3. Fill them in (greedy or stochastic)
        for i in range(num_steps):
            t_curr = timesteps[i].expand(batch_size)
            t_next = timesteps[i + 1] if i < num_steps - 1 else torch.zeros_like(t_curr)
            
            xt = self.sample_step(xt, t_curr, t_next, product_mask, temperature)
        
        # Extract generated products
        # For each sequence in the batch:
        # Take tokens starting from product_start
        # Skip PAD
        # Stop at EOS
        # Collect the rest as final product tokens
        generated_products = []
        for i in range(batch_size):
            product_start = product_starts[i]
            # Extract product tokens (everything after product_start until padding/EOS)
            product_tokens = xt[i, product_start:]
            
            # Remove padding, mask tokens, and find EOS
            valid_tokens = []
            for token in product_tokens:
                token_id = token.item()
                # Skip [PAD] and [MASK]
                if token_id in [self.config.pad_token_id, self.config.mask_token_id]:
                    continue
                # Stop at [EOS]
                elif token_id == self.config.eos_token_id:
                    break
                else:
                    # Keep token as part of the predicted output
                    valid_tokens.append(token_id)
            # Save the cleaned-up product tokens for this sequence
            generated_products.append(torch.tensor(valid_tokens, device=device))
        
        return generated_products
    
    def generate_product(self, reactants: str, agents: str, tokenizer,
                        num_steps: int = 100, temperature: float = 1.0) -> str:
        """
        Generate a single reaction product.
        
        Args:
            reactants: Reactant SMILES string
            agents: Agent SMILES string (can be empty)
            tokenizer: CustomTokenizer instance
            num_steps: Number of diffusion steps
            temperature: Sampling temperature
            
        Returns:
            Generated product SMILES string
        """
        generated_products = self.sample(
            [reactants], [agents], tokenizer, num_steps, temperature
        )
        
        if generated_products and len(generated_products[0]) > 0:
            product_tokens = generated_products[0].cpu().tolist()
            # Decode to SMILES string
            return tokenizer.decode(product_tokens)
        else:
            return ""

class ReactionDataset(Dataset):
    """Dataset for reaction SMILES strings."""
    
    def __init__(self, data: List[str], tokenizer, config: ReactionMD4Config):
        self.data = data
        self.tokenizer = tokenizer
        self.config = config

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Grab one reaction string from the dataset
        reaction_str = self.data[idx].strip()
        
        # Preprocess reaction string into training inputs
        # - x_masked: input tokens where some positions are masked (diffusion step)
        # - x_target: ground-truth tokens
        # - product_mask: boolean mask indicating product token positions
        x_masked, x_target, product_mask = preprocess_reaction_for_training(
            reaction_str, self.tokenizer, self.config
        )
        
        return {
            "x_masked": x_masked,
            "x_target": x_target,
            "product_mask": product_mask
        }

def calculate_accuracy_fixed(predictions: torch.Tensor, targets: torch.Tensor, 
                            mask: torch.Tensor, pad_token_id: int, 
                            noisy_input: torch.Tensor, mask_token_id: int) -> Dict[str, float]:
    """Calculate token-wise and sequence-wise accuracy"""
    
    # Only consider positions that are:
    # 1. In the product mask (product region)
    # 2. Not padding tokens  
    # 3. Actually masked in the noisy input (what the model needs to predict)
    actually_masked = (noisy_input == mask_token_id)
    valid_positions = mask & (targets != pad_token_id) & actually_masked
    
    if not valid_positions.any():
        return {"token_accuracy": 0.0, "sequence_accuracy": 0.0}
    
    # Token-wise accuracy - only on positions the model actually predicted
    correct_tokens = (predictions == targets) & valid_positions # True exactly where prediction equals target and the position is valid
    token_accuracy = correct_tokens.sum().float() / valid_positions.sum().float() # number of correct valid tokens divided by number of valid tokens
    
    # Sequence-wise accuracy - all masked tokens in each sequence must be correct
    batch_size = predictions.shape[0]
    sequence_correct = torch.zeros(batch_size, dtype=torch.bool, device=predictions.device)
    
    for i in range(batch_size):
        seq_mask = valid_positions[i]
        if seq_mask.any():
            seq_correct = (predictions[i] == targets[i]) & seq_mask
            sequence_correct[i] = seq_correct.sum() == seq_mask.sum()
        else:
            # If no tokens to predict, consider it "correct"
            sequence_correct[i] = True
    
    sequence_accuracy = sequence_correct.float().mean()
    
    return {
        "token_accuracy": token_accuracy.item(),
        "sequence_accuracy": sequence_accuracy.item(),
        "num_predicted_tokens": valid_positions.sum().item()
    }


def calculate_generation_accuracy(model: ReactionMD4, val_dataset, tokenizer, 
                                num_samples: int = 50) -> Dict[str, float]:
    """Calculate accuracy on actual generation task."""
    model.eval()
    device = next(model.parameters()).device
    
    correct_products = 0
    total_samples = 0
    
    # Sample random indices
    sample_indices = random.sample(range(len(val_dataset)), min(num_samples, len(val_dataset)))
    
    with torch.no_grad():
        for idx in sample_indices:
            try:
                sample = val_dataset[idx]
                x_target = sample['x_target']
                
                # Decode target to get components
                original_reaction = tokenizer.decode(x_target.tolist())
                parts = original_reaction.split('>')

                if len(parts) == 3 and parts[1] == "":
                    reactants, true_product = parts[0], parts[2]
                    agents = tokenizer.noagent_token
                elif len(parts) == 3:
                    reactants, agents, true_product = parts
                elif len(parts) == 2:
                    reactants, true_product = parts
                    agents = tokenizer.noagent_token
                else:
                    continue
                
                # Generate product
                generated_product = model.generate_product(
                    reactants, agents, tokenizer, num_steps=50, temperature=1.0
                )
                
                # Compare (exact match)
                if generated_product.strip() == true_product.strip():
                    correct_products += 1
                
                total_samples += 1
                
            except Exception as e:
                continue
    
    generation_accuracy = correct_products / max(total_samples, 1)
    return {
        "generation_accuracy": generation_accuracy,
        "correct_samples": correct_products,
        "total_samples": total_samples
    }

@torch.no_grad()
def test_model(model: ReactionMD4, test_dataset, tokenizer, 
               batch_size: int = 32, num_steps: int = 100):
    """Test the model on test dataset and return comprehensive metrics."""
    model.eval()
    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Use batch_size=1 for simplicity
    
    all_token_accuracies = []
    all_seq_accuracies = []
    all_generated_products = []
    all_true_products = []
    all_inputs = []
    
    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="Testing")):
        x_target = batch['x_target'][0]  # Remove batch dimension
        
        # Decode the full target reaction
        original_reaction = tokenizer.decode(x_target.tolist())
        
        # Parse reaction to extract components
        parts = original_reaction.split('>')
        if len(parts) == 3 and parts[1] == "":
            reactants, true_product = parts[0], parts[2]
            agents = tokenizer.noagent_token
        elif len(parts) == 3:
            reactants, agents, true_product = parts
        elif len(parts) == 2:
            reactants, true_product = parts
            agents = tokenizer.noagent_token
        else:
            continue
        
        # Generate product
        generated_product = model.generate_product(
            reactants, agents, tokenizer, num_steps=num_steps, temperature=1.0
        )
        
        # Calculate accuracy at character level (simple metric)
        true_product_clean = true_product.strip()
        generated_product_clean = generated_product.strip()
        
        if true_product_clean:
            # Sequence accuracy
            seq_accuracy = 1.0 if generated_product_clean == true_product_clean else 0.0
            
            # Token-level token accuracy
            if generated_product_clean:
                true_tokens = tokenizer.encode(true_product_clean)
                gen_tokens = tokenizer.encode(generated_product_clean)
                
                # Calculate token accuracy without padding
                min_len = min(len(true_tokens), len(gen_tokens))
                max_len = max(len(true_tokens), len(gen_tokens))
                
                # Count correct tokens only up to the shorter sequence (so padded tokens are not considered in the accuracy)
                correct_tokens = sum(1 for i in range(min_len) if true_tokens[i] == gen_tokens[i])
                
                # Use the length of the true sequence as denominator
                # This penalizes both missing tokens and extra tokens
                token_accuracy = correct_tokens / len(true_tokens)
            else:
                token_accuracy = 0.0
            
            all_token_accuracies.append(token_accuracy)
            all_seq_accuracies.append(seq_accuracy)
            
            # Store for detailed analysis
            all_inputs.append(original_reaction)
            all_true_products.append(true_product_clean)
            all_generated_products.append(generated_product_clean)
        
        if (batch_idx + 1) % 100 == 0:
            print(f"  Processed {batch_idx + 1}/{len(test_dataloader)} samples")
    
    # Calculate final metrics
    if all_token_accuracies:
        final_token_acc = np.mean(all_token_accuracies)
        final_seq_acc = np.mean(all_seq_accuracies)
        
        print(f"\n{'='*50}")
        print("FINAL TEST RESULTS")
        print(f"{'='*50}")
        print(f"Token Accuracy: {final_token_acc:.4f}")
        print(f"Sequence Accuracy: {final_seq_acc:.4f}")
        print(f"Total Test Samples: {len(all_token_accuracies)}")
        
        # Show some examples
        print(f"\nSample Test Results:")
        num_examples = min(5, len(all_inputs))
        for i in range(num_examples):
            print(f"\nExample {i+1}:")
            # Extract input part (reactants+agents)
            parts = all_inputs[i].split('>')
            if len(parts) >= 2:
                input_part = '>'.join(parts[:-1]) + '>'
                print(f"  Input:      {input_part}")
            print(f"  True:       {all_true_products[i]}")
            print(f"  Generated:  {all_generated_products[i]}")
            print(f"  Match:      {'match' if all_seq_accuracies[i] == 1.0 else 'not match'}")
        
        return {
            "token_accuracy": final_token_acc,
            "sequence_accuracy": final_seq_acc,
            "num_samples": len(all_token_accuracies),
            "examples": {
                "inputs": all_inputs[:10],
                "true_products": all_true_products[:10],
                "generated_products": all_generated_products[:10]
            }
        }
    else:
        print("No valid test samples found!")
        return {"token_accuracy": 0.0, "sequence_accuracy": 0.0, "num_samples": 0}

@dataclass
class TrainingMetrics:
    """Container for training metrics."""
    epoch: int
    train_loss: float
    train_token_acc: float
    train_seq_acc: float
    val_loss: float = None
    val_token_acc: float = None
    val_seq_acc: float = None
    learning_rate: float = None
    timestamp: str = None
    generated_examples: List[Dict] = None

class MetricsTracker:
    """Track and store training metrics with visualization."""
    
    def __init__(self, save_dir: str = "training_logs_fixed_ACC"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        self.metrics_history = []
        self.generated_examples_history = []
        
    def add_metrics(self, metrics: TrainingMetrics):
        """Add metrics for current epoch."""
        metrics.timestamp = datetime.now().isoformat()
        self.metrics_history.append(metrics)
        
    def add_generated_examples(self, epoch: int, examples: List[Dict]):
        """Add generated examples for current epoch."""
        self.generated_examples_history.append({
            "epoch": epoch,
            "timestamp": datetime.now().isoformat(),
            "examples": examples
        })
        
    def plot_metrics(self, save_path: str = None):
        """Create comprehensive training plots."""
        if not self.metrics_history:
            print("No metrics to plot!")
            return
            
        # Extract data
        epochs = [m.epoch for m in self.metrics_history]
        train_losses = [m.train_loss for m in self.metrics_history]
        train_token_accs = [m.train_token_acc for m in self.metrics_history]
        train_seq_accs = [m.train_seq_acc for m in self.metrics_history]
        
        # Validation metrics (may be sparse)
        val_epochs = [m.epoch for m in self.metrics_history if m.val_loss is not None]
        val_losses = [m.val_loss for m in self.metrics_history if m.val_loss is not None]
        val_token_accs = [m.val_token_acc for m in self.metrics_history if m.val_token_acc is not None]
        val_seq_accs = [m.val_seq_acc for m in self.metrics_history if m.val_seq_acc is not None]
        
        # Create figure with subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Metrics Dashboard', fontsize=16, fontweight='bold')
        
        # Plot 1: Loss
        ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2, alpha=0.8)
        if val_losses:
            ax1.plot(val_epochs, val_losses, 'r-', label='Val Loss', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training & Validation Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Token Accuracy
        ax2.plot(epochs, train_token_accs, 'b-', label='Train Token Acc', linewidth=2, alpha=0.8)
        if val_token_accs:
            ax2.plot(val_epochs, val_token_accs, 'r-', label='Val Token Acc', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Token Accuracy')
        ax2.set_title('Token-Level Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1)
        
        # Plot 3: Sequence Accuracy
        ax3.plot(epochs, train_seq_accs, 'b-', label='Train Seq Acc', linewidth=2, alpha=0.8)
        if val_seq_accs:
            ax3.plot(val_epochs, val_seq_accs, 'r-', label='Val Seq Acc', linewidth=2, alpha=0.8, marker='o', markersize=4)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Sequence Accuracy')
        ax3.set_title('Sequence-Level Accuracy')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
        
        # Plot 4: Learning Rate
        learning_rates = [m.learning_rate for m in self.metrics_history if m.learning_rate is not None]
        if learning_rates:
            lr_epochs = [m.epoch for m in self.metrics_history if m.learning_rate is not None]
            ax4.plot(lr_epochs, learning_rates, 'g-', linewidth=2, alpha=0.8)
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Learning Rate')
            ax4.set_title('Learning Rate Schedule')
            ax4.set_yscale('log')
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'No LR data', ha='center', va='center', transform=ax4.transAxes)
            ax4.set_title('Learning Rate Schedule')
        
        plt.tight_layout()
        
        # Save plot
        if save_path is None:
            save_path = os.path.join(self.save_dir, f"training_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Metrics plot saved to: {save_path}")
        
        # Also display
        plt.show()
        
    def save_metrics_json(self, filename: str = None):
        """Save all metrics to JSON file."""
        if filename is None:
            filename = os.path.join(self.save_dir, f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
        # Convert to serializable format
        metrics_data = []
        for m in self.metrics_history:
            metric_dict = {
                "epoch": m.epoch,
                "train_loss": m.train_loss,
                "train_token_acc": m.train_token_acc,
                "train_seq_acc": m.train_seq_acc,
                "val_loss": m.val_loss,
                "val_token_acc": m.val_token_acc,
                "val_seq_acc": m.val_seq_acc,
                "learning_rate": m.learning_rate,
                "timestamp": m.timestamp
            }
            metrics_data.append(metric_dict)
            
        with open(filename, 'w') as f:
            json.dump(metrics_data, f, indent=2)
        print(f"Metrics saved to: {filename}")
        
    def save_generated_examples(self, filename: str = None):
        """Save generated examples to JSON file."""
        if filename is None:
            filename = os.path.join(self.save_dir, f"generated_examples_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
        with open(filename, 'w') as f:
            json.dump(self.generated_examples_history, f, indent=2)
        print(f"Generated examples saved to: {filename}")

class GeneratedExampleSampler:
    """Smart sampling of generated examples during training."""
    
    def __init__(self, total_dataset_size: int, max_examples_per_epoch: int = 50):
        self.total_dataset_size = total_dataset_size
        self.max_examples_per_epoch = max_examples_per_epoch
        
        # For 4.5M samples, this gives reasonable coverage
        self.sampling_strategy = self._determine_sampling_strategy()
        
    def _determine_sampling_strategy(self):
        """Determine optimal sampling strategy based on dataset size."""
        if self.total_dataset_size < 10000:
            return "random_subset"  # Small dataset - random sampling
        elif self.total_dataset_size < 100000:
            return "stratified"     # Medium dataset - stratified sampling
        else:
            return "systematic"     # Large dataset - systematic sampling
            
    def sample_indices(self, dataset_size: int, epoch: int) -> List[int]:
        """Sample indices for generating examples."""
        if self.sampling_strategy == "random_subset":
            return random.sample(range(dataset_size), min(self.max_examples_per_epoch, dataset_size))
        
        elif self.sampling_strategy == "stratified":
            # Sample from different parts of the dataset
            n_samples = min(self.max_examples_per_epoch, dataset_size)
            indices = []
            for i in range(n_samples):
                # Sample from different strata
                stratum_idx = (i * dataset_size) // n_samples
                offset = random.randint(0, dataset_size // n_samples - 1)
                idx = min(stratum_idx + offset, dataset_size - 1)
                indices.append(idx)
            return indices
            
        else:  # systematic
            # Systematic sampling with random start, rotated by epoch
            step = max(1, dataset_size // self.max_examples_per_epoch)
            start = (epoch * 17) % step  # Use prime number to avoid patterns
            return list(range(start, dataset_size, step))[:self.max_examples_per_epoch]
        
def enhanced_generate_examples_during_training(model, val_dataset, tokenizer, 
                                             metrics_tracker, epoch, example_sampler,
                                             num_examples: int = 50):
    """Generate examples AND calculate true generation accuracy."""
    
    # Call your existing example generation function
    generated_examples = generate_examples_during_training_enhanced(
        model, val_dataset, tokenizer, metrics_tracker, epoch, 
        example_sampler, num_examples
    )

    if generated_examples:
        metrics_tracker.add_generated_examples(epoch, generated_examples)
    
    # Calculate actual generation accuracy on a separate set of samples
    print(f"\n CALCULATING TRUE GENERATION ACCURACY...")
    gen_results = calculate_generation_accuracy(
        model, val_dataset, tokenizer, num_samples=25  # Smaller sample for speed
    )
    
    print(f"      TRUE Generation Accuracy: {gen_results['generation_accuracy']:.3f}")
    print(f"      Correct: {gen_results['correct_samples']}/{gen_results['total_samples']}")
    
    # return results and examples
    return {
        "generation_accuracy": gen_results["generation_accuracy"],
        "correct_samples": gen_results["correct_samples"],
        "total_samples": gen_results["total_samples"],
        "generated_examples": generated_examples
    }

def truncate_at_eos(sequence_str: str, tokenizer) -> str:
    """Truncate a decoded sequence at the first EOS token."""
    # If the sequence contains the EOS token representation, truncate there
    tokens = tokenizer.encode(sequence_str)
    eos_positions = [i for i, t in enumerate(tokens) if t == tokenizer.eos_token_id]
    
    if eos_positions:
        # Re-decode only up to the first EOS
        tokens_before_eos = tokens[:eos_positions[0]]
        return tokenizer.decode(tokens_before_eos)
    return sequence_str

def generate_examples_during_training_enhanced(model, dataset, tokenizer, 
                                             metrics_tracker: MetricsTracker,
                                             epoch: int, sampler: GeneratedExampleSampler,
                                             num_examples: int = 50):
    """Enhanced example generation with intelligent sampling and storage."""
    model.eval()
    device = next(model.parameters()).device
    
    # Get sample indices using smart sampling
    sample_indices = sampler.sample_indices(len(dataset), epoch)[:num_examples]
    
    generated_examples = []
    
    with torch.no_grad():
        for i, idx in enumerate(sample_indices):
            try:
                sample = dataset[idx]
                x_target = sample['x_target']
                
                # Decode the full target reaction
                original_reaction = tokenizer.decode(x_target.tolist())
                
                # Parse reaction to extract reactants and agents
                parts = original_reaction.split('>')
                if len(parts) == 3 and parts[1] == "":
                    reactants, true_product = parts[0], parts[2]
                    agents = tokenizer.noagent_token
                elif len(parts) == 3:
                    reactants, agents, true_product = parts
                elif len(parts) == 2:
                    reactants, true_product = parts
                    agents = tokenizer.noagent_token
                else:
                    continue
                
                # Generate product with different temperatures for diversity
                temperatures = [0.8, 1.0, 1.2]
                generated_variants = []
                
                for temp in temperatures:
                    generated_product = model.generate_product(
                        reactants, agents, tokenizer, num_steps=50, temperature=temp
                    )
                    generated_variants.append({
                        "temperature": temp,
                        "product": generated_product.strip(),
                        "is_correct": generated_product.strip() == true_product.strip()
                    })
                
                # Store example
                example_data = {
                    "sample_idx": idx,
                    "reactants": reactants,
                    "agents": agents,
                    "true_product": true_product.strip(),
                    "generated_variants": generated_variants,
                    "input_format": f"{reactants}>{agents}>" if agents else f"{reactants}>>"
                }
                
                generated_examples.append(example_data)
                
                # Print first few examples for monitoring
                if i < 3:
                    reactants_agents_text = f"{reactants}>{agents}>" if agents else f"{reactants}>>"
                    best_match = max(generated_variants, key=lambda x: x["is_correct"])
                    match_indicator = "Correct" if best_match["is_correct"] else "Not correct"
                    
                    print(f"      {i+1}. {reactants_agents_text}")
                    print(f"         True:      {true_product}")
                    print(f"         Generated: {best_match['product']} {match_indicator}")
                    if i < 2:
                        print()
                        
            except Exception as e:
                print(f"      Error generating example {i+1}: {str(e)}")

    return generated_examples
        
def train_reaction_md4_with_elbo(model, train_dataset, val_dataset, tokenizer,
                               num_epochs: int = 10, batch_size: int = 32,
                               learning_rate: float = 1e-4, validation_freq: int = 1,
                               save_dir: str = "training_logs_elbo",
                               examples_per_epoch: int = 50,
                               use_elbo_loss: bool = True):
    """Training loop with ELBO loss support."""
    
    device = next(model.parameters()).device
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))
    
    # Initialize tracking
    metrics_tracker = MetricsTracker(save_dir)
    example_sampler = GeneratedExampleSampler(len(train_dataset), examples_per_epoch)
    
    best_val_loss = float('inf')
    
    print(f"Starting training with {'ELBO' if use_elbo_loss else 'Standard'} loss for {num_epochs} epochs...")
    print(f"Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}")
    print(f"Saving metrics and examples to: {save_dir}")
    print("=" * 80)
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_token_acc = 0
        train_seq_acc = 0
        num_successful_batches = 0
        
        # ELBO specific metrics
        total_importance_weight = 0
        total_masked_tokens = 0
        
        pbar_train = tqdm(
            train_dataloader, 
            desc=f"Epoch {epoch+1}/{num_epochs} [Training]",
            ncols=120,
            leave=True,
            position=0
        )
        
        for batch_idx, batch in enumerate(pbar_train):
            x_target = batch['x_target'].to(device)
            product_mask = batch['product_mask'].to(device)
            
            batch_size_curr = x_target.shape[0]
            
            if use_elbo_loss and hasattr(model, 'elbo_loss'):
                # ELBO loss handles timestep sampling internally
                # We pass the target and let ELBO loss handle everything
                elbo_results = model.elbo_loss.compute_elbo_loss(
                    model=model,
                    x_target=x_target,
                    product_mask=product_mask,
                    use_importance_sampling=True
                )
                
                loss = elbo_results["loss"]
                
                # Track ELBO-specific metrics
                if elbo_results["num_masked_tokens"] > 0:
                    total_importance_weight += elbo_results["mean_importance_weight"]
                    total_masked_tokens += elbo_results["num_masked_tokens"]
                
                # For accuracy calculation, we need the noisy input that was generated
                # This is done internally in ELBO, so we need to recalculate for metrics
                if product_mask.any():
                    with torch.no_grad():
                        # Sample t and create noisy input for accuracy calculation
                        t = torch.rand(batch_size_curr, device=device)
                        x_noisy = model.forward_sample(x_target, t, product_mask)
                        outputs = model(x_noisy, t)
                        logits = outputs["logits"]
                        predictions = torch.argmax(logits, dim=-1)
                        
                        acc_metrics = calculate_accuracy_fixed(
                            predictions, x_target, product_mask, 
                            model.config.pad_token_id, x_noisy, model.config.mask_token_id
                        )
                        train_token_acc += acc_metrics["token_accuracy"]
                        train_seq_acc += acc_metrics["sequence_accuracy"]
                
            else:
                # Original training logic (non-ELBO)
                t = torch.rand(batch_size_curr, device=device)
                x_noisy = model.forward_sample(x_target, t, product_mask)
                
                outputs = model(x_noisy, t, x_target, product_mask, use_elbo_loss=False)
                loss = outputs.get('loss', torch.tensor(0.0, device=device, requires_grad=True))
                
                if product_mask.any():
                    with torch.no_grad():
                        logits = outputs["logits"]
                        predictions = torch.argmax(logits, dim=-1)
                        
                        acc_metrics = calculate_accuracy_fixed(
                            predictions, x_target, product_mask, 
                            model.config.pad_token_id, x_noisy, model.config.mask_token_id
                        )
                        train_token_acc += acc_metrics["token_accuracy"]
                        train_seq_acc += acc_metrics["sequence_accuracy"]
            
            # Skip if no valid loss
            if loss.item() <= 0 or not loss.requires_grad:
                continue
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            num_successful_batches += 1
            
            # Update progress bar
            if num_successful_batches > 0:
                avg_train_loss = train_loss / num_successful_batches
                avg_token_acc = train_token_acc / num_successful_batches
                avg_seq_acc = train_seq_acc / num_successful_batches
                
                postfix_dict = {
                    "loss": f"{avg_train_loss:.4f}",
                    "tok_acc": f"{avg_token_acc:.3f}",
                    "seq_acc": f"{avg_seq_acc:.3f}",
                    "lr": f"{scheduler.get_last_lr()[0]:.2e}"
                }
                
                # Add ELBO-specific metrics if available
                if use_elbo_loss and num_successful_batches > 0:
                    avg_importance_weight = total_importance_weight / num_successful_batches
                    postfix_dict["iw"] = f"{avg_importance_weight:.2f}"
                
                pbar_train.set_postfix(postfix_dict)
        
        pbar_train.close()
        
        # Calculate final training metrics
        if num_successful_batches > 0:
            avg_train_loss = train_loss / num_successful_batches
            avg_train_token_acc = train_token_acc / num_successful_batches
            avg_train_seq_acc = train_seq_acc / num_successful_batches
        else:
            avg_train_loss = avg_train_token_acc = avg_train_seq_acc = 0.0
        
        # Create metrics object
        epoch_metrics = TrainingMetrics(
            epoch=epoch + 1,
            train_loss=avg_train_loss,
            train_token_acc=avg_train_token_acc,
            train_seq_acc=avg_train_seq_acc,
            learning_rate=scheduler.get_last_lr()[0]
        )
        
        # Validation phase
        if (epoch + 1) % validation_freq == 0:
            model.eval()
            val_loss = 0
            val_token_acc = 0
            val_seq_acc = 0
            num_val_batches = 0
            val_importance_weight = 0
            
            pbar_val = tqdm(
                val_dataloader, 
                desc=f"Epoch {epoch+1}/{num_epochs} [Validation]",
                ncols=120,
                leave=True,
                position=0
            )
            
            with torch.no_grad():
                for batch in pbar_val:
                    x_target = batch['x_target'].to(device)
                    product_mask = batch['product_mask'].to(device)
                    
                    batch_size_curr = x_target.shape[0]
                    
                    if use_elbo_loss and hasattr(model, 'elbo_loss'):
                        # Use ELBO loss for validation
                        elbo_results = model.elbo_loss.compute_elbo_loss(
                            model=model,
                            x_target=x_target,
                            product_mask=product_mask,
                            use_importance_sampling=True
                        )
                        
                        loss = elbo_results["loss"]
                        
                        if elbo_results["num_masked_tokens"] > 0:
                            val_importance_weight += elbo_results["mean_importance_weight"]
                        
                        # Calculate accuracy metrics
                        if product_mask.any():
                            t = torch.rand(batch_size_curr, device=device)
                            x_noisy = model.forward_sample(x_target, t, product_mask)
                            outputs = model(x_noisy, t)
                            logits = outputs["logits"]
                            predictions = torch.argmax(logits, dim=-1)
                            
                            acc_metrics = calculate_accuracy_fixed(
                                predictions, x_target, product_mask, 
                                model.config.pad_token_id, x_noisy, model.config.mask_token_id
                            )
                            val_token_acc += acc_metrics["token_accuracy"]
                            val_seq_acc += acc_metrics["sequence_accuracy"]
                    
                    else:
                        # Original validation logic
                        t = torch.rand(batch_size_curr, device=device)
                        x_noisy = model.forward_sample(x_target, t, product_mask)
                        
                        outputs = model(x_noisy, t, x_target, product_mask, use_elbo_loss=False)
                        loss = outputs.get('loss', torch.tensor(0.0, device=device))
                        
                        if product_mask.any():
                            logits = outputs["logits"]
                            predictions = torch.argmax(logits, dim=-1)
                            
                            acc_metrics = calculate_accuracy_fixed(
                                predictions, x_target, product_mask, 
                                model.config.pad_token_id, x_noisy, model.config.mask_token_id
                            )
                            val_token_acc += acc_metrics["token_accuracy"]
                            val_seq_acc += acc_metrics["sequence_accuracy"]
                    
                    val_loss += loss.item()
                    num_val_batches += 1
                    
                    # Update validation progress bar
                    if num_val_batches > 0:
                        avg_val_loss = val_loss / num_val_batches
                        avg_val_token_acc = val_token_acc / num_val_batches
                        avg_val_seq_acc = val_seq_acc / num_val_batches
                        
                        postfix_dict = {
                            "loss": f"{avg_val_loss:.4f}",
                            "tok_acc": f"{avg_val_token_acc:.3f}",
                            "seq_acc": f"{avg_val_seq_acc:.3f}"
                        }
                        
                        if use_elbo_loss and num_val_batches > 0:
                            avg_val_iw = val_importance_weight / num_val_batches
                            postfix_dict["iw"] = f"{avg_val_iw:.2f}"
                        
                        pbar_val.set_postfix(postfix_dict)
            
            pbar_val.close()
            
            # Update metrics with validation results
            if num_val_batches > 0:
                epoch_metrics.val_loss = val_loss / num_val_batches
                epoch_metrics.val_token_acc = val_token_acc / num_val_batches
                epoch_metrics.val_seq_acc = val_seq_acc / num_val_batches
            
            # Print epoch summary
            print(f"\n EPOCH {epoch+1} SUMMARY:")
            print(f"Training: Loss: {avg_train_loss:.4f} | Token Acc: {avg_train_token_acc:.3f} | Seq Acc: {avg_train_seq_acc:.3f}")
            print(f"Validation: Loss: {epoch_metrics.val_loss:.4f} | Token Acc: {epoch_metrics.val_token_acc:.3f} | Seq Acc: {epoch_metrics.val_seq_acc:.3f}")
            
            if use_elbo_loss and num_successful_batches > 0:
                avg_train_iw = total_importance_weight / num_successful_batches
                avg_val_iw = val_importance_weight / num_val_batches if num_val_batches > 0 else 0
                print(f"Importance Weights - Train: {avg_train_iw:.2f} | Val: {avg_val_iw:.2f}")
            
            print(f"LR: {scheduler.get_last_lr()[0]:.2e}")
            
            # Check for best model
            if epoch_metrics.val_loss < best_val_loss:
                best_val_loss = epoch_metrics.val_loss
                print(f"NEW BEST VALIDATION LOSS! ({epoch_metrics.val_loss:.4f})")
                
                # Save best model
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'config': model.config,
                    'val_loss': best_val_loss,
                    'metrics_history': metrics_tracker.metrics_history,
                    'use_elbo_loss': use_elbo_loss
                }, os.path.join(save_dir, 'best_model_elbo.pt'))
            
            # Generate examples
            print(f"\n Generated examples:")
            gen_results = enhanced_generate_examples_during_training(
                model, val_dataset, tokenizer, metrics_tracker, epoch + 1, 
                example_sampler, examples_per_epoch
            )
        
        # Add metrics to tracker
        metrics_tracker.add_metrics(epoch_metrics)
        
        # Save metrics periodically
        if (epoch + 1) % 5 == 0:
            metrics_tracker.save_metrics_json()
            metrics_tracker.save_generated_examples()
            
        # Plot metrics periodically
        if (epoch + 1) % 5 == 0:
            metrics_tracker.plot_metrics()
            
        print("=" * 80)
    
    print(f"\n TRAINING COMPLETED!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Loss type used: {'ELBO' if use_elbo_loss else 'Standard'}")
    
    # Final saves
    metrics_tracker.save_metrics_json()
    metrics_tracker.save_generated_examples()
    
    # Final comprehensive plot
    print("\n Creating final metrics visualization...")
    metrics_tracker.plot_metrics(os.path.join(save_dir, "final_training_metrics.png"))
    
    return metrics_tracker
