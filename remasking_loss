import torch
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, List

class ReMDMELBOLoss:
    """
    ELBO Loss implementation for ReMasking Discrete Diffusion Models (ReMDM).
    Implements the confidence-based remasking schedule from the ReMDM paper.
    
    ELBO: sum_{t=1}^T E_q_sigma[((1-sigma_t)*alpha_t - alpha_s)/(1-alpha_t) * log <x_theta(z_t), x>]
    """
    
    def __init__(self, config):
        """
        Args:
            config: Config object with attributes:
                - timesteps: int
                - mask_token_id: int
                - pad_token_id: int
                - noise_schedule: str ("linear", "cosine", "geometric")
                - base_sigma: float (base remasking probability, default 0.5)
                - temperature: float (for confidence score softmax, default 1.0)
                - sampling_sampler: str (sampling method, e.g., 'remdm-conf')
        """
        self.config = config
        self.mask_token_id = config.mask_token_id
        self.pad_token_id = config.pad_token_id
        self.timesteps = config.timesteps
        self.base_sigma = getattr(config, 'base_sigma', 0.5)
        self.temperature = getattr(config, 'temperature', 1.0)
        
        # Pre-compute alpha schedule
        self.alphas = self._build_alpha_schedule()
        
        # Store confidence scores for each position
        self.confidence_scores = {}
        
    def _build_alpha_schedule(self) -> torch.Tensor:
        """
        Build alpha schedule based on config.
        Returns tensor of shape (timesteps + 1,) where alpha[0] = 1.0
        """
        t = torch.linspace(0, 1, self.timesteps + 1)
        
        if self.config.noise_schedule == "linear":
            alphas = 1 - t
        elif self.config.noise_schedule == "cosine":
            alphas = torch.cos(torch.pi / 2 * t)
        else:
            # Default to cosine
            alphas = torch.cos(torch.pi / 2 * t)
        
        # Ensure alpha[0] = 1.0 (no noise at t=0)
        alphas[0] = 1.0
        
        return alphas
    
    def compute_confidence_based_sigma_github_style(
        self,
        x_noisy: torch.Tensor,
        model_logits: torch.Tensor,
        product_mask: torch.Tensor,
        alpha_t: float,
        alpha_s: float,
        conf: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute confidence-based sigma and sampling probabilities using GitHub-style approach.
        
        Args:
            x_noisy: Current noisy sequence [batch, seq_len]
            model_logits: Model predictions [batch, seq_len, vocab_size]
            product_mask: Boolean mask for product positions [batch, seq_len]
            alpha_t: Current timestep alpha value
            alpha_s: Previous timestep alpha value  
            conf: Current confidence scores [batch, seq_len]
            
        Returns:
            sigma: Per-token remasking probabilities [batch, seq_len]
            q_xs: Sampling probabilities [batch, seq_len, vocab_size]
        """
        batch_size, seq_len, vocab_size = model_logits.shape
        device = x_noisy.device
        
        # Get model probabilities
        p_x0 = F.softmax(model_logits, dim=-1) 
        
        # Compute sigma_max
        if alpha_t > 0:
            sigma_max = min(1.0, (1 - alpha_s) / alpha_t)
        else:
            sigma_max = 1.0
        
        # Compute eta (normalized confidence scores)
        eta = F.softmax(conf, dim=-1)
        
        # Zero out confidence for already masked positions
        masked_flag = (x_noisy == self.mask_token_id).to(torch.bool)
        # set their confidence weight to 0
        eta[masked_flag] = 0
        
        # Compute sigma per token
        sigma = eta * sigma_max
        
        # Compute q_xs (probability distribution for sampling)
        # For unmasked tokens: q_xs = p_x0 * (1 - sigma)
        # With prob. 1-sigma keep the models distributions
        # With prob. sigma replace with mask token
        q_xs = p_x0 * (1 - sigma.unsqueeze(-1))
        q_xs[..., self.mask_token_id] = sigma  # Set mask token probability to sigma
        
        # Compute q_xs_2 (alternative distribution for masked tokens)
        if alpha_t > 0:
            weight_factor = (alpha_s - (1 - sigma.unsqueeze(-1)) * alpha_t) / (1 - alpha_t)
            q_xs_2 = p_x0 * weight_factor
            mask_prob_2 = (1 - alpha_s - sigma * alpha_t) / (1 - alpha_t)
            q_xs_2[..., self.mask_token_id] = mask_prob_2.unsqueeze(-1)
        else:
            q_xs_2 = q_xs.clone()
        
        # Select between q_xs and q_xs_2 based on whether token is currently masked
        # If token is not masked use q_xs
        # If it is masked usw q_xs_2
        copy_flag = (x_noisy != self.mask_token_id).to(torch.bool)
        q_xs = torch.where(copy_flag.unsqueeze(-1), q_xs, q_xs_2)
        
        # Ensure probabilities are non-negative and normalized
        q_xs = torch.clamp(q_xs, min=1e-10)
        q_xs = q_xs / q_xs.sum(dim=-1, keepdim=True)
        
        return sigma, q_xs
    
    def sample_categorical(self, probs: torch.Tensor) -> torch.Tensor:
        """
        Sample from categorical distribution.
        
        Args:
            probs: Probabilities [batch, seq_len, vocab_size]
            
        Returns:
            samples: Sampled tokens [batch, seq_len]
        """
        # Flatten for sampling
        batch_size, seq_len, vocab_size = probs.shape
        # Reshape so we can call multinomial once for all points
        probs_flat = probs.reshape(-1, vocab_size)
        
        # Sample tokens using torch.multinomial (sample one token per position)
        samples_flat = torch.multinomial(probs_flat, num_samples=1).squeeze(-1)
        
        # Reshape back (each entry is a sampled token ID)
        samples = samples_flat.reshape(batch_size, seq_len)
        
        return samples
    
    def update_confidence_scores(
        self,
        x_prev: torch.Tensor,
        x_new: torch.Tensor,
        p_x0: torch.Tensor,
        conf: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            x_prev: Previous sequence [batch, seq_len]
            x_new: New sampled sequence [batch, seq_len]
            p_x0: Model probabilities [batch, seq_len, vocab_size]
            conf: Current confidence scores [batch, seq_len]
            
        Returns:
            updated_conf: Updated confidence scores [batch, seq_len]
        """
        batch_size, seq_len = x_prev.shape
        device = x_prev.device
        
        # Create batch and sequence indices (use for indexing per sequence and per position)
        batch_indices = torch.arange(batch_size, device=device)[:, None]
        feature_indices = torch.arange(seq_len, device=device)
        
        # Update confidence for newly unmasked tokens
        # Get positions, where we need to assign a new confidence score, so was masked before
        # and is now sampled into a real token
        unmask_mask = (x_prev == self.mask_token_id) & (x_new != self.mask_token_id)
        # Get the models predicted probability for the new token at each position
        conf_values = -p_x0[batch_indices, feature_indices, x_new]  # Negative log prob -> lower prob. means lower confidence
        # update conf for those newly unmasked tokens
        conf[unmask_mask] = conf_values[unmask_mask]
        
        # Set confidence to -inf for newly remasked tokens
        remask_mask = (x_prev != self.mask_token_id) & (x_new == self.mask_token_id)
        conf[remask_mask] = float('-inf')
        
        return conf
    

    def compute_confidence_based_sigma(
        self,
        x_noisy: torch.Tensor,
        model_logits: torch.Tensor,
        product_mask: torch.Tensor,
        batch_idx: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute confidence-based sigma schedule for each token.
        
        Args:
            x_noisy: Current noisy sequence [batch, seq_len]
            model_logits: Model predictions [batch, seq_len, vocab_size]
            product_mask: Boolean mask for product positions [batch, seq_len]
            batch_idx: Optional batch indices for tracking confidence
            
        Returns:
            sigma: Per-token remasking probabilities [batch, seq_len]
        """
        batch_size, seq_len = x_noisy.shape
        device = x_noisy.device
        
        # Initialize sigma to zeros
        sigma = torch.zeros((batch_size, seq_len), device=device)
        
        # Get probabilities from logits
        probs = F.softmax(model_logits / self.temperature, dim=-1)
        
        # For unmasked tokens, compute confidence-based sigma
        is_unmasked = (x_noisy != self.mask_token_id) & product_mask
        
        if is_unmasked.any():
            # Get the predicted probability for the actual token
            batch_indices = torch.arange(batch_size, device=device).unsqueeze(1)
            seq_indices = torch.arange(seq_len, device=device).unsqueeze(0)
            
            # Extract probabilities for actual tokens where unmasked
            token_probs = probs[batch_indices, seq_indices, x_noisy]
            
            # FIXED: Compute sigma directly from confidence
            # Low confidence (prob near 0) -> high sigma (near base_sigma)
            # High confidence (prob near 1) -> low sigma (near 0)
            confidence_based_sigma = self.base_sigma * (1.0 - token_probs)
            
            # Apply only to unmasked tokens
            sigma[is_unmasked] = confidence_based_sigma[is_unmasked]
            
            # Alternative approach using exponential decay:
            # sigma[is_unmasked] = self.base_sigma * torch.exp(-token_probs[is_unmasked] / self.temperature)
            
            # Store confidence scores for tracking (optional)
            """ if batch_idx is not None:
                for b in range(batch_size):
                    key = batch_idx[b].item() if batch_idx is not None else b
                    if key not in self.confidence_scores:
                        self.confidence_scores[key] = {}
                    
                    for s in range(seq_len):
                        if is_unmasked[b, s]:
                            self.confidence_scores[key][(b, s)] = token_probs[b, s].item() """
        
        # Ensure masked tokens have sigma = 0 (they shouldn't be remasked)
        sigma[~is_unmasked] = 0.0
        
        return sigma
    
    def compute_elbo_weights(
        self,
        t_indices: torch.Tensor,
        sigma: torch.Tensor,
        device: torch.device
    ) -> torch.Tensor:
        """
        Compute ELBO weights for ReMDM.
        
        w(t) = ((1 - sigma_t) * alpha_t - alpha_{t-1}) / (1 - alpha_t)
        
        Args:
            t_indices: Timestep indices [batch] (ranging from 1 to timesteps)
            sigma: Per-token remasking probabilities [batch, seq_len]
            device: Device to use
            
        Returns:
            weights: Per-token weights [batch, seq_len]
        """
        alphas = self.alphas.to(device)
        
        # Ensure t_indices are valid (should be between 1 and timesteps)
        t_indices = t_indices.clamp(min=1, max=self.timesteps)
        
        # Get alpha values
        # alpha_t corresponds to current timestep
        alpha_t = alphas[t_indices].unsqueeze(1)  # [batch, 1]
        # alpha_s corresponds to previous timestep (less noisy)
        # When t=1, we want alpha_0 which should be 1.0 (no noise)
        alpha_s = alphas[t_indices - 1].unsqueeze(1)  # [batch, 1]
        
        # Compute denominator with stability
        denominator = 1.0 - alpha_t + 1e-8
        
        # Compute weights: ((1 - sigma) * alpha_t - alpha_s) / (1 - alpha_t), but change the 
        # sign just like in the ELBO computation
        weights = (alpha_s - (1.0 - sigma) * alpha_t) / denominator
        
        # Clamp for stability
        weights = torch.clamp(weights, min=0.0, max=100.0)
        
        return weights
    
    def remask_sequence_github_style(
        self,
        x_current: torch.Tensor,
        q_xs: torch.Tensor,
        product_mask: torch.Tensor,
        conf: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply remasking using GitHub-style categorical sampling.
        
        Args:
            x_current: Current sequence [batch, seq_len]
            q_xs: Sampling probabilities [batch, seq_len, vocab_size]
            product_mask: Valid positions mask [batch, seq_len]
            conf: Current confidence scores [batch, seq_len]
            
        Returns:
            x_remasked: Sequence after remasking [batch, seq_len]
            updated_conf: Updated confidence scores [batch, seq_len]
        """
        # Sample new tokens
        x_new = self.sample_categorical(q_xs)
        
        # Only apply changes to valid positions
        x_remasked = torch.where(product_mask, x_new, x_current)
        
        # Update confidence scores
        p_x0 = q_xs  # Using q_xs as probability estimates
        # If a new token was chosen, assign new confidence based on prob. of that token
        # If a token was remasked, set confidence to -inf
        updated_conf = self.update_confidence_scores(x_current, x_remasked, p_x0, conf.clone())
        
        return x_remasked, updated_conf
    
    def remask_sequence(
        self,
        x_current: torch.Tensor,
        sigma: torch.Tensor,
        product_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        Apply remasking based on sigma probabilities (original method).
        Kept for backward compatibility.
        """
        # Sample remasking decisions
        remask_prob = torch.rand_like(sigma)
        should_remask = (remask_prob < sigma) & product_mask
        
        # Apply remasking
        # If should_remask is true, replace token with [MASK]
        # If false, keep original token
        x_remasked = torch.where(should_remask, self.mask_token_id, x_current)

        # Add debugging in remask_sequence:
        """ print(f"Sigma stats: min={sigma.min()}, max={sigma.max()}, mean={sigma.mean()}")
        print(f"Should remask any: {should_remask.any()}") """
        
        return x_remasked
    
    def compute_elbo_loss(
        self,
        model=None,
        x_target: torch.Tensor = None,
        product_mask: torch.Tensor = None,
        num_samples: int = 1,
        use_github_style: bool = False,
        # For testing allow precomputed values
        precomputed_logits: torch.Tensor = None,
        precomputed_t: torch.Tensor = None,
        precomputed_x_noisy: torch.Tensor = None,
        precomputed_sigma: torch.Tensor = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute ELBO loss for ReMDM with confidence-based remasking.
        
        Args:
            model: The diffusion model (optional if precomputed_logits provided)
            x_target: Original sequence [batch, seq_len]
            product_mask: Boolean mask for product positions [batch, seq_len]
            num_samples: Number of samples for Monte Carlo estimation
            use_github_style: Whether to use GitHub-style confidence sampling
            precomputed_logits: For testing - pre-computed model outputs
            precomputed_t: For testing - pre-computed timesteps
            precomputed_x_noisy: For testing - pre-computed noisy sequences
            precomputed_sigma: For testing - pre-computed sigma values
            
        Returns:
            Dictionary containing loss and diagnostic information
        """
        batch_size, seq_len = x_target.shape
        device = x_target.device
        
        # Move alphas to device
        alphas = self.alphas.to(device)
        
        # Use precomputed or sample timesteps
        if precomputed_t is not None:
            t = precomputed_t
            if t.max() <= 1.0:
                t_indices = (t * self.timesteps).long().clamp(1, self.timesteps)
            else:
                t_indices = t.long()
        else:
            # Sample timesteps uniformly
            t_indices = torch.randint(1, self.timesteps + 1, (batch_size,), device=device)
            t = t_indices.float() / self.timesteps
        
        # Initialize confidence scores
        if use_github_style:
            # get per token confidencce, start with all zeros
            conf = torch.zeros((batch_size, seq_len), device=device)
        
        # Initialize loss accumulator for Monte Carlo estimation
        total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        total_diagnostics = {
            "num_masked_tokens": 0,
            "num_remasked_tokens": 0,
            "mean_sigma": 0.0,
            "mean_weight": 0.0
        }
        loss_samples = []
        
        for sample_idx in range(num_samples):
            # Forward diffusion:
            # Use precomputed or generate noisy sequence
            if precomputed_x_noisy is not None:
                x_noisy = precomputed_x_noisy
            else:
                # Forward diffusion process
                alpha_t = alphas[t_indices]
                
                # Initial masking based on alpha_t
                mask_prob = torch.rand_like(x_target, dtype=torch.float32)
                should_mask = (mask_prob > alpha_t.unsqueeze(-1)) & product_mask
                x_noisy = torch.where(should_mask, self.mask_token_id, x_target)
            
            # Use precomputed or get model predictions
            if precomputed_logits is not None:
                logits = precomputed_logits
            else:
                if model is None:
                    raise ValueError("Either model or precomputed_logits must be provided")
                outputs = model(x_noisy, t)
                logits = outputs["logits"]
            
            # Choose remasking strategy
            if use_github_style and getattr(self.config, 'sampling_sampler', None) == 'remdm-conf':
                # Github-style confidence-based sampling
                alpha_t_val = alphas[t_indices[0]].item()
                alpha_s_val = alphas[t_indices[0] - 1].item() if t_indices[0] > 1 else 1.0
                # Compute per token sigma and categorical distrib. q_xs
                sigma, q_xs = self.compute_confidence_based_sigma_github_style(
                    x_noisy, logits, product_mask, alpha_t_val, alpha_s_val, conf
                )
                # Sample a new sequence and update conf
                x_remasked, conf = self.remask_sequence_github_style(
                    x_noisy, q_xs, product_mask, conf
                )
            else:
                # Original sigma-based remasking
                if precomputed_sigma is not None:
                    sigma = precomputed_sigma
                    if (sigma == 0).all():
                        x_remasked = x_noisy
                    else:
                        x_remasked = self.remask_sequence(x_noisy, sigma, product_mask)
                else:
                    sigma = self.compute_confidence_based_sigma(
                        x_noisy, logits, product_mask
                    )
                    x_remasked = self.remask_sequence(x_noisy, sigma, product_mask)
            
            # Compute ELBO weights
            # Fot the test case with all maked set weights to 1
            if precomputed_sigma is not None and (x_noisy == self.mask_token_id).all():
                weights = torch.ones((batch_size, seq_len), device=device)
            else:
                weights = self.compute_elbo_weights(t_indices, sigma, device)
            
            # Identify positions to compute loss on
            # Only compute loss on tokens that are still masked and in product region
            is_currently_masked = (x_remasked == self.mask_token_id)
            valid_positions = (is_currently_masked & product_mask)
            
            # Track which tokens were initially masked and which ones got remasked
            is_originally_masked = (x_noisy == self.mask_token_id) & product_mask
            is_remasked = (x_noisy != self.mask_token_id) & (x_remasked == self.mask_token_id)
            
            # Handle edge cases
            # If nothing is masked return 0 loss
            if not valid_positions.any():
                return {
                    "loss": torch.tensor(0.0, device=device, requires_grad=True),
                    "num_masked_tokens": 0,
                    "num_remasked_tokens": 0,
                    "mean_sigma": 0.0,
                    "mean_weight": 0.0,
                    "t_mean": t.float().mean().item()
                }
            
            # Compute cross-entropy loss
            logits_flat = logits.reshape(-1, logits.size(-1))
            targets_flat = x_target.reshape(-1)
            valid_flat = valid_positions.reshape(-1)
            weights_flat = weights.reshape(-1)
            
            # Get valid logits, targets, and weights
            valid_logits = logits_flat[valid_flat]
            valid_targets = targets_flat[valid_flat]
            valid_weights = weights_flat[valid_flat]

            if not valid_positions.any():
                loss_samples.append(torch.tensor(0.0, device=device))
                continue
            
            # Compute weighted token-level loss
            token_losses = F.cross_entropy(valid_logits, valid_targets, reduction='none')
            weighted_token_losses = valid_weights * token_losses
                        
            # Aggregate loss
            sample_loss = weighted_token_losses.mean()
            loss_samples.append(sample_loss)
            
            # Update diagnostics
            total_diagnostics["num_masked_tokens"] += is_originally_masked.sum().item()
            total_diagnostics["num_remasked_tokens"] += is_remasked.sum().item()
            total_diagnostics["mean_sigma"] += sigma[product_mask].mean().item()
            total_diagnostics["mean_weight"] += weights[product_mask].mean().item()
        
        # Average over samples
        if loss_samples:
            final_loss = torch.stack(loss_samples).mean()
        else:
            final_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # Average diagnostics
        for key in total_diagnostics:
            total_diagnostics[key] /= max(num_samples, 1)
        
        return {
            "loss": final_loss,
            "num_masked_tokens": total_diagnostics["num_masked_tokens"],
            "num_remasked_tokens": total_diagnostics["num_remasked_tokens"],
            "mean_sigma": total_diagnostics["mean_sigma"],
            "mean_weight": total_diagnostics["mean_weight"],
            "t_mean": t.float().mean().item()
        }
